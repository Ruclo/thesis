diff --git a/tests/virt/snapshot/conftest.py b/tests/virt/snapshot/conftest.py
new file mode 100644
index 0000000..f904b4c
--- /dev/null
+++ b/tests/virt/snapshot/conftest.py
@@ -0,0 +1,486 @@
+# -*- coding: utf-8 -*-
+
+"""
+Shared fixtures for VM snapshot restore tests with runStrategy RerunOnFailure.
+
+This module provides common fixtures for VM snapshot and restore testing,
+including VM creation with different run strategies, snapshot creation,
+and data validation utilities.
+"""
+
+import logging
+import shlex
+
+import pytest
+from ocp_resources.datavolume import DataVolume
+from ocp_resources.virtual_machine import VirtualMachine
+from ocp_resources.virtual_machine_snapshot import VirtualMachineSnapshot
+from pyhelper_utils.shell import run_ssh_commands
+
+from utilities.constants import OS_FLAVOR_CIRROS, TIMEOUT_10MIN, Images
+from utilities.storage import create_dv, create_vm_from_dv
+from utilities.virt import running_vm
+
+LOGGER = logging.getLogger(__name__)
+
+
+@pytest.fixture()
+def vm_with_rerun_on_failure(
+    namespace,
+    unprivileged_client,
+    storage_class_for_snapshot,
+):
+    """
+    Create a VM with runStrategy: RerunOnFailure.
+
+    Yields:
+        VirtualMachine: VM resource with RerunOnFailure run strategy
+    """
+    vm_name = "vm-rerun-on-failure"
+    dv_name = f"{vm_name}-dv"
+
+    LOGGER.info(f"Creating DataVolume {dv_name} for VM with RerunOnFailure strategy")
+
+    with create_dv(
+        dv_name=dv_name,
+        namespace=namespace.name,
+        storage_class=storage_class_for_snapshot,
+        url=Images.Cirros.QCOW2_IMG_GZ,
+        size="1Gi",
+        client=unprivileged_client,
+    ) as dv:
+        dv.wait_for_dv_success(timeout=TIMEOUT_10MIN)
+
+        LOGGER.info(f"Creating VM {vm_name} with runStrategy: RerunOnFailure")
+        with create_vm_from_dv(
+            vm_name=vm_name,
+            dv=dv,
+            start=False,
+            client=unprivileged_client,
+        ) as vm:
+            # Set runStrategy to RerunOnFailure
+            vm.instance.spec.runStrategy = VirtualMachine.RunStrategy.RERUNONFAILURE
+            vm.update()
+
+            yield vm
+
+
+@pytest.fixture()
+def vm_with_run_strategy(
+    namespace,
+    unprivileged_client,
+    storage_class_for_snapshot,
+    request,
+):
+    """
+    Create a VM with specified run strategy.
+
+    Args:
+        request.param (str): Run strategy (Always, Manual, Halted, RerunOnFailure, Once)
+
+    Yields:
+        VirtualMachine: VM resource with specified run strategy
+    """
+    run_strategy = request.param
+    vm_name = f"vm-{run_strategy.lower()}"
+    dv_name = f"{vm_name}-dv"
+
+    LOGGER.info(f"Creating DataVolume {dv_name} for VM with {run_strategy} strategy")
+
+    with create_dv(
+        dv_name=dv_name,
+        namespace=namespace.name,
+        storage_class=storage_class_for_snapshot,
+        url=Images.Cirros.QCOW2_IMG_GZ,
+        size="1Gi",
+        client=unprivileged_client,
+    ) as dv:
+        dv.wait_for_dv_success(timeout=TIMEOUT_10MIN)
+
+        LOGGER.info(f"Creating VM {vm_name} with runStrategy: {run_strategy}")
+        with create_vm_from_dv(
+            vm_name=vm_name,
+            dv=dv,
+            start=False,
+            client=unprivileged_client,
+        ) as vm:
+            # Set runStrategy
+            vm.instance.spec.runStrategy = run_strategy
+            vm.update()
+
+            yield vm
+
+
+@pytest.fixture()
+def running_vm_with_rerun_on_failure(vm_with_rerun_on_failure):
+    """
+    Start VM with RerunOnFailure and wait for Running state.
+
+    Yields:
+        VirtualMachine: Running VM with RerunOnFailure strategy
+    """
+    vm = vm_with_rerun_on_failure
+    LOGGER.info(f"Starting VM {vm.name}")
+    vm.start(wait=True)
+    running_vm(vm=vm)
+    LOGGER.info(f"VM {vm.name} is running")
+    yield vm
+
+
+@pytest.fixture()
+def vm_snapshot(running_vm_with_rerun_on_failure):
+    """
+    Create VirtualMachineSnapshot from running VM.
+
+    Yields:
+        VirtualMachineSnapshot: Ready snapshot resource
+    """
+    vm = running_vm_with_rerun_on_failure
+    snapshot_name = f"{vm.name}-snapshot"
+
+    LOGGER.info(f"Creating VirtualMachineSnapshot {snapshot_name} for VM {vm.name}")
+
+    with VirtualMachineSnapshot(
+        name=snapshot_name,
+        namespace=vm.namespace,
+        vm_name=vm.name,
+    ) as snapshot:
+        LOGGER.info(f"Waiting for snapshot {snapshot_name} to be ready")
+        snapshot.wait_ready_to_use(timeout=TIMEOUT_10MIN)
+        LOGGER.info(f"Snapshot {snapshot_name} is ready")
+        yield snapshot
+
+
+@pytest.fixture()
+def stopped_vm_with_snapshot(running_vm_with_rerun_on_failure, vm_snapshot):
+    """
+    VM with snapshot, in stopped state.
+
+    Yields:
+        tuple: (VirtualMachine, VirtualMachineSnapshot) - VM stopped, snapshot ready
+    """
+    vm = running_vm_with_rerun_on_failure
+    snapshot = vm_snapshot
+
+    LOGGER.info(f"Stopping VM {vm.name}")
+    vm.stop(wait=True)
+    LOGGER.info(f"VM {vm.name} is stopped")
+
+    yield vm, snapshot
+
+
+@pytest.fixture()
+def running_vm_with_snapshot(running_vm_with_rerun_on_failure, vm_snapshot):
+    """
+    Running VM with snapshot created.
+
+    Yields:
+        tuple: (VirtualMachine, VirtualMachineSnapshot) - VM running, snapshot ready
+    """
+    vm = running_vm_with_rerun_on_failure
+    snapshot = vm_snapshot
+    yield vm, snapshot
+
+
+@pytest.fixture()
+def vm_with_data_and_snapshot_restored(
+    running_vm_with_rerun_on_failure,
+    vm_snapshot,
+):
+    """
+    VM with test data written before snapshot, modified after, and then restored.
+
+    Yields:
+        tuple: (VirtualMachine, str, str) - Running restored VM, test file path, expected content
+    """
+    vm = running_vm_with_rerun_on_failure
+    snapshot = vm_snapshot
+    test_file = "/data/before-snapshot.txt"
+    after_file = "/data/after-snapshot.txt"
+    expected_content = "original-data"
+
+    # Create directory and write file before snapshot was taken
+    # (snapshot was already created in vm_snapshot fixture)
+    # This means we need to write the file, create snapshot, then write another file
+
+    # Actually, we need to restructure this - snapshot is already created
+    # Let's write files in proper order for this test
+
+    LOGGER.info("Writing test data files for snapshot restore test")
+
+    # Create directory
+    run_ssh_commands(host=vm.ssh_exec, commands=[shlex.split("mkdir -p /data")])
+
+    # Write before-snapshot file
+    run_ssh_commands(
+        host=vm.ssh_exec,
+        commands=[shlex.split(f"echo '{expected_content}' > {test_file}")],
+    )
+
+    # Create a new snapshot with the data
+    snapshot_with_data_name = f"{vm.name}-snapshot-with-data"
+    with VirtualMachineSnapshot(
+        name=snapshot_with_data_name,
+        namespace=vm.namespace,
+        vm_name=vm.name,
+    ) as data_snapshot:
+        data_snapshot.wait_ready_to_use(timeout=TIMEOUT_10MIN)
+
+        # Write after-snapshot file
+        run_ssh_commands(
+            host=vm.ssh_exec,
+            commands=[shlex.split(f"echo 'new-data' > {after_file}")],
+        )
+
+        # Stop VM
+        vm.stop(wait=True)
+
+        # Restore from snapshot
+        from ocp_resources.virtual_machine_restore import VirtualMachineRestore
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore-data",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=data_snapshot.name,
+        ) as restore:
+            restore.wait_restore_done(timeout=TIMEOUT_10MIN)
+
+    # Start VM after restore
+    vm.start(wait=True)
+    running_vm(vm=vm)
+
+    yield vm, test_file, expected_content
+
+
+@pytest.fixture()
+def vm_with_run_strategy_and_snapshot(
+    vm_with_run_strategy,
+):
+    """
+    VM with specified run strategy and snapshot created.
+
+    Yields:
+        tuple: (VirtualMachine, VirtualMachineSnapshot)
+    """
+    vm = vm_with_run_strategy
+    run_strategy = vm.instance.spec.runStrategy
+
+    # Start VM if strategy allows (not Halted)
+    if run_strategy != VirtualMachine.RunStrategy.HALTED:
+        vm.start(wait=True)
+        running_vm(vm=vm)
+
+    snapshot_name = f"{vm.name}-snapshot"
+    with VirtualMachineSnapshot(
+        name=snapshot_name,
+        namespace=vm.namespace,
+        vm_name=vm.name,
+    ) as snapshot:
+        snapshot.wait_ready_to_use(timeout=TIMEOUT_10MIN)
+
+        # Stop VM if it's running
+        if run_strategy != VirtualMachine.RunStrategy.HALTED:
+            vm.stop(wait=True)
+
+        yield vm, snapshot
+
+
+@pytest.fixture()
+def vm_with_always_strategy_stopped_with_snapshot(
+    namespace,
+    unprivileged_client,
+    storage_class_for_snapshot,
+):
+    """
+    VM with runStrategy: Always, snapshot created, VM stopped.
+
+    Yields:
+        tuple: (VirtualMachine, VirtualMachineSnapshot)
+    """
+    vm_name = "vm-always"
+    dv_name = f"{vm_name}-dv"
+
+    with create_dv(
+        dv_name=dv_name,
+        namespace=namespace.name,
+        storage_class=storage_class_for_snapshot,
+        url=Images.Cirros.QCOW2_IMG_GZ,
+        size="1Gi",
+        client=unprivileged_client,
+    ) as dv:
+        dv.wait_for_dv_success(timeout=TIMEOUT_10MIN)
+
+        with create_vm_from_dv(
+            vm_name=vm_name,
+            dv=dv,
+            start=False,
+            client=unprivileged_client,
+        ) as vm:
+            vm.instance.spec.runStrategy = VirtualMachine.RunStrategy.ALWAYS
+            vm.update()
+
+            vm.start(wait=True)
+            running_vm(vm=vm)
+
+            with VirtualMachineSnapshot(
+                name=f"{vm_name}-snapshot",
+                namespace=vm.namespace,
+                vm_name=vm.name,
+            ) as snapshot:
+                snapshot.wait_ready_to_use(timeout=TIMEOUT_10MIN)
+                vm.stop(wait=True)
+                yield vm, snapshot
+
+
+@pytest.fixture()
+def vm_with_manual_strategy_stopped_with_snapshot(
+    namespace,
+    unprivileged_client,
+    storage_class_for_snapshot,
+):
+    """
+    VM with runStrategy: Manual, snapshot created, VM stopped.
+
+    Yields:
+        tuple: (VirtualMachine, VirtualMachineSnapshot)
+    """
+    vm_name = "vm-manual"
+    dv_name = f"{vm_name}-dv"
+
+    with create_dv(
+        dv_name=dv_name,
+        namespace=namespace.name,
+        storage_class=storage_class_for_snapshot,
+        url=Images.Cirros.QCOW2_IMG_GZ,
+        size="1Gi",
+        client=unprivileged_client,
+    ) as dv:
+        dv.wait_for_dv_success(timeout=TIMEOUT_10MIN)
+
+        with create_vm_from_dv(
+            vm_name=vm_name,
+            dv=dv,
+            start=False,
+            client=unprivileged_client,
+        ) as vm:
+            vm.instance.spec.runStrategy = VirtualMachine.RunStrategy.MANUAL
+            vm.update()
+
+            vm.start(wait=True)
+            running_vm(vm=vm)
+
+            with VirtualMachineSnapshot(
+                name=f"{vm_name}-snapshot",
+                namespace=vm.namespace,
+                vm_name=vm.name,
+            ) as snapshot:
+                snapshot.wait_ready_to_use(timeout=TIMEOUT_10MIN)
+                vm.stop(wait=True)
+                yield vm, snapshot
+
+
+@pytest.fixture()
+def vm_with_halted_strategy_with_snapshot(
+    namespace,
+    unprivileged_client,
+    storage_class_for_snapshot,
+):
+    """
+    VM with runStrategy: Halted and snapshot created (VM never started).
+
+    Yields:
+        tuple: (VirtualMachine, VirtualMachineSnapshot)
+    """
+    vm_name = "vm-halted"
+    dv_name = f"{vm_name}-dv"
+
+    with create_dv(
+        dv_name=dv_name,
+        namespace=namespace.name,
+        storage_class=storage_class_for_snapshot,
+        url=Images.Cirros.QCOW2_IMG_GZ,
+        size="1Gi",
+        client=unprivileged_client,
+    ) as dv:
+        dv.wait_for_dv_success(timeout=TIMEOUT_10MIN)
+
+        with create_vm_from_dv(
+            vm_name=vm_name,
+            dv=dv,
+            start=False,
+            client=unprivileged_client,
+        ) as vm:
+            vm.instance.spec.runStrategy = VirtualMachine.RunStrategy.HALTED
+            vm.update()
+
+            # Do not start VM - create snapshot of halted VM
+            with VirtualMachineSnapshot(
+                name=f"{vm_name}-snapshot",
+                namespace=vm.namespace,
+                vm_name=vm.name,
+            ) as snapshot:
+                snapshot.wait_ready_to_use(timeout=TIMEOUT_10MIN)
+                yield vm, snapshot
+
+
+@pytest.fixture()
+def vm_with_multiple_snapshots(
+    running_vm_with_rerun_on_failure,
+):
+    """
+    VM with multiple snapshots created at different points.
+
+    Yields:
+        tuple: (VirtualMachine, VirtualMachineSnapshot, VirtualMachineSnapshot)
+    """
+    vm = running_vm_with_rerun_on_failure
+    file1 = "/data/snapshot1.txt"
+    file2 = "/data/snapshot2.txt"
+
+    # Create directory
+    run_ssh_commands(host=vm.ssh_exec, commands=[shlex.split("mkdir -p /data")])
+
+    # Write first file and create first snapshot
+    LOGGER.info("Creating first snapshot")
+    run_ssh_commands(
+        host=vm.ssh_exec,
+        commands=[shlex.split(f"echo 'data1' > {file1}")],
+    )
+
+    with VirtualMachineSnapshot(
+        name=f"{vm.name}-snap1",
+        namespace=vm.namespace,
+        vm_name=vm.name,
+    ) as snap1:
+        snap1.wait_ready_to_use(timeout=TIMEOUT_10MIN)
+
+        # Write second file and create second snapshot
+        LOGGER.info("Creating second snapshot")
+        run_ssh_commands(
+            host=vm.ssh_exec,
+            commands=[shlex.split(f"echo 'data2' > {file2}")],
+        )
+
+        with VirtualMachineSnapshot(
+            name=f"{vm.name}-snap2",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+        ) as snap2:
+            snap2.wait_ready_to_use(timeout=TIMEOUT_10MIN)
+
+            # Stop VM
+            vm.stop(wait=True)
+
+            yield vm, snap1, snap2
+
+
+@pytest.fixture()
+def snapshot_capable_storage_class(admin_client, storage_class_for_snapshot):
+    """
+    Get or verify snapshot-capable storage class.
+
+    Returns:
+        str: Name of storage class supporting VolumeSnapshots
+    """
+    return storage_class_for_snapshot
diff --git a/tests/virt/snapshot/test_snapshot_restore_rerun_on_failure.py b/tests/virt/snapshot/test_snapshot_restore_rerun_on_failure.py
new file mode 100644
index 0000000..41a62de
--- /dev/null
+++ b/tests/virt/snapshot/test_snapshot_restore_rerun_on_failure.py
@@ -0,0 +1,879 @@
+# -*- coding: utf-8 -*-
+
+"""
+VM Snapshot Restore with runStrategy RerunOnFailure Tests
+
+STP Reference: ../thesis/stps/4.md
+Jira: https://issues.redhat.com/browse/CNV-63819
+
+This module contains tests for verifying snapshot restore functionality
+when VMs use runStrategy: RerunOnFailure. Tests verify that the restore
+operation completes successfully without getting stuck due to the VM
+auto-starting during the restore process.
+"""
+
+import logging
+import shlex
+
+import pytest
+from ocp_resources.virtual_machine import VirtualMachine
+from ocp_resources.virtual_machine_instance import VirtualMachineInstance
+from ocp_resources.virtual_machine_restore import VirtualMachineRestore
+from ocp_resources.virtual_machine_snapshot import VirtualMachineSnapshot
+from pyhelper_utils.shell import run_ssh_commands
+from timeout_sampler import TimeoutExpiredError, TimeoutSampler
+
+from utilities.constants import TIMEOUT_5MIN, TIMEOUT_10MIN
+from utilities.virt import running_vm
+
+LOGGER = logging.getLogger(__name__)
+
+
+pytestmark = pytest.mark.usefixtures(
+    "skip_if_no_storage_class_for_snapshot",
+)
+
+
+class TestSnapshotRestoreRerunOnFailure:
+    """
+    Tests for VM snapshot restore with runStrategy: RerunOnFailure.
+
+    This class contains tests verifying the CNV-63819 bug fix ensuring snapshot
+    restore completes successfully for VMs using RerunOnFailure run strategy.
+    """
+
+    @pytest.mark.polarion("CNV-63819-01")
+    @pytest.mark.gating
+    def test_restore_completes_with_rerun_on_failure(
+        self,
+        stopped_vm_with_snapshot,
+    ):
+        """
+        Test that snapshot restore completes successfully for VM with runStrategy: RerunOnFailure.
+
+        This is the primary test case for CNV-63819. The bug reports that snapshot restore
+        gets stuck indefinitely because virt-controller tries to start the VM during restore.
+
+        Steps:
+            1. Create VirtualMachineRestore resource targeting the snapshot
+            2. Wait for restore operation to complete (timeout: 5 minutes)
+            3. Check VirtualMachineRestore status
+
+        Expected:
+            - VirtualMachineRestore status is "Complete"
+        """
+        vm, snapshot = stopped_vm_with_snapshot
+        LOGGER.info(f"Creating VirtualMachineRestore for VM {vm.name} from snapshot {snapshot.name}")
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as vm_restore:
+            LOGGER.info(f"Waiting for restore {vm_restore.name} to complete")
+            vm_restore.wait_restore_done(timeout=TIMEOUT_5MIN)
+
+            LOGGER.info("Verifying VirtualMachineRestore status")
+            assert vm_restore.instance.status.complete, (
+                f"VirtualMachineRestore {vm_restore.name} did not complete. "
+                f"Status: {vm_restore.instance.status}"
+            )
+            LOGGER.info(f"Restore {vm_restore.name} completed successfully")
+
+    @pytest.mark.polarion("CNV-63819-02")
+    @pytest.mark.gating
+    def test_vm_does_not_start_during_restore(
+        self,
+        stopped_vm_with_snapshot,
+    ):
+        """
+        Test that VM does not auto-start while restore is in progress.
+
+        This verifies the core issue in CNV-63819 - virt-controller should NOT
+        attempt to start the VM while restore is ongoing.
+
+        Steps:
+            1. Create VirtualMachineRestore resource targeting the snapshot
+            2. During restore operation, query for VirtualMachineInstance resources
+            3. Monitor virt-controller logs for VM start attempts
+
+        Expected:
+            - No VirtualMachineInstance exists during restore
+        """
+        vm, snapshot = stopped_vm_with_snapshot
+        LOGGER.info(f"Creating VirtualMachineRestore for VM {vm.name} from snapshot {snapshot.name}")
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as vm_restore:
+            LOGGER.info("Checking that no VMI exists during restore operation")
+
+            # Poll for restore completion while verifying no VMI exists
+            sampler = TimeoutSampler(
+                wait_timeout=TIMEOUT_5MIN,
+                sleep=5,
+                func=lambda: vm_restore.instance.status.get("complete", False),
+            )
+
+            for sample in sampler:
+                # Check for VMI existence during restore
+                try:
+                    vmi = VirtualMachineInstance(name=vm.name, namespace=vm.namespace)
+                    if vmi.exists:
+                        pytest.fail(
+                            f"VMI {vmi.name} exists during restore operation. "
+                            "VM should not auto-start during restore."
+                        )
+                except Exception:
+                    # VMI doesn't exist - this is expected
+                    pass
+
+                if sample:
+                    LOGGER.info("Restore completed without VM auto-starting")
+                    break
+
+    @pytest.mark.polarion("CNV-63819-03")
+    def test_manual_start_succeeds_after_restore(
+        self,
+        stopped_vm_with_snapshot,
+    ):
+        """
+        Test that VM can be manually started after restore completes.
+
+        Steps:
+            1. Start the VM manually
+            2. Wait for VM to reach Running state
+
+        Expected:
+            - VM is "Running"
+        """
+        vm, snapshot = stopped_vm_with_snapshot
+        LOGGER.info(f"Creating VirtualMachineRestore for VM {vm.name}")
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as vm_restore:
+            LOGGER.info("Waiting for restore to complete")
+            vm_restore.wait_restore_done(timeout=TIMEOUT_5MIN)
+
+        LOGGER.info(f"Starting VM {vm.name} manually after restore")
+        vm.start(wait=True)
+        running_vm(vm=vm)
+
+        LOGGER.info(f"VM {vm.name} is running after restore")
+        assert vm.instance.status.printableStatus == VirtualMachine.Status.RUNNING
+
+    @pytest.mark.polarion("CNV-63819-04")
+    def test_restore_preserves_vm_configuration(
+        self,
+        stopped_vm_with_snapshot,
+    ):
+        """
+        Test that restored VM has the same configuration as the snapshot.
+
+        Steps:
+            1. Query restored VM specification
+            2. Compare vCPU count, memory, and labels with original configuration
+
+        Expected:
+            - VM vCPU count equals 2
+        """
+        vm, snapshot = stopped_vm_with_snapshot
+        LOGGER.info(f"Recording original VM {vm.name} configuration")
+
+        # Get original configuration
+        original_cpus = vm.instance.spec.template.spec.domain.cpu.cores
+        original_memory = vm.instance.spec.template.spec.domain.resources.requests.get("memory")
+        original_run_strategy = vm.instance.spec.runStrategy
+
+        LOGGER.info(
+            f"Original config - CPUs: {original_cpus}, Memory: {original_memory}, "
+            f"RunStrategy: {original_run_strategy}"
+        )
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as vm_restore:
+            LOGGER.info("Waiting for restore to complete")
+            vm_restore.wait_restore_done(timeout=TIMEOUT_5MIN)
+
+        LOGGER.info("Verifying VM configuration after restore")
+        vm.wait_for_status(status=VirtualMachine.Status.STOPPED, timeout=TIMEOUT_5MIN)
+
+        # Verify configuration is preserved
+        restored_cpus = vm.instance.spec.template.spec.domain.cpu.cores
+        restored_memory = vm.instance.spec.template.spec.domain.resources.requests.get("memory")
+        restored_run_strategy = vm.instance.spec.runStrategy
+
+        assert restored_cpus == original_cpus, (
+            f"CPU count mismatch after restore: expected {original_cpus}, got {restored_cpus}"
+        )
+        assert restored_memory == original_memory, (
+            f"Memory mismatch after restore: expected {original_memory}, got {restored_memory}"
+        )
+        assert restored_run_strategy == original_run_strategy, (
+            f"RunStrategy mismatch after restore: expected {original_run_strategy}, "
+            f"got {restored_run_strategy}"
+        )
+        LOGGER.info("VM configuration preserved correctly after restore")
+
+
+class TestSnapshotRestoreDataIntegrity:
+    """
+    Tests for data integrity during snapshot restore with RerunOnFailure.
+    """
+
+    @pytest.mark.polarion("CNV-63819-05")
+    @pytest.mark.tier2
+    def test_restore_preserves_data_written_before_snapshot(
+        self,
+        vm_with_data_and_snapshot_restored,
+    ):
+        """
+        Test that files created before snapshot are preserved after restore.
+
+        Steps:
+            1. SSH to restored VM
+            2. Read file /data/before-snapshot.txt
+
+        Expected:
+            - File content equals "original-data"
+        """
+        vm, test_file, expected_content = vm_with_data_and_snapshot_restored
+
+        LOGGER.info(f"Verifying file {test_file} exists and has correct content")
+        cmd = shlex.split(f"cat {test_file}")
+        result = run_ssh_commands(host=vm.ssh_exec, commands=cmd)[0]
+
+        assert result.strip() == expected_content, (
+            f"File content mismatch: expected '{expected_content}', got '{result.strip()}'"
+        )
+        LOGGER.info(f"File {test_file} correctly preserved after restore")
+
+    @pytest.mark.polarion("CNV-63819-06")
+    @pytest.mark.tier2
+    def test_restore_removes_data_written_after_snapshot(
+        self,
+        vm_with_data_and_snapshot_restored,
+    ):
+        """
+        Test that files created after snapshot are removed after restore.
+
+        Steps:
+            1. SSH to restored VM
+            2. Check if file /data/after-snapshot.txt exists
+
+        Expected:
+            - File /data/after-snapshot.txt does NOT exist
+        """
+        vm, _, _ = vm_with_data_and_snapshot_restored
+        after_file = "/data/after-snapshot.txt"
+
+        LOGGER.info(f"Verifying file {after_file} does not exist after restore")
+        cmd = shlex.split(f"test -f {after_file} && echo exists || echo not_exists")
+        result = run_ssh_commands(host=vm.ssh_exec, commands=cmd)[0]
+
+        assert result.strip() == "not_exists", (
+            f"File {after_file} should not exist after restore but was found"
+        )
+        LOGGER.info(f"File {after_file} correctly removed after restore")
+
+    @pytest.mark.polarion("CNV-63819-07")
+    @pytest.mark.tier2
+    def test_complete_restore_workflow_with_data_validation(
+        self,
+        vm_with_rerun_on_failure,
+        skip_if_no_storage_class_for_snapshot,
+    ):
+        """
+        Test complete end-to-end snapshot restore workflow with data validation.
+
+        This is a comprehensive Tier 2 test covering the full user workflow.
+
+        Steps:
+            1. Start VM and wait for Running state
+            2. SSH to VM and write test data to /data/test-file.txt
+            3. Create VirtualMachineSnapshot
+            4. Wait for snapshot Ready state
+            5. SSH to VM and write different data to /data/modified.txt
+            6. Stop VM
+            7. Create VirtualMachineRestore
+            8. Wait for restore Complete state
+            9. Start VM manually
+            10. Wait for VM Running state
+            11. SSH to VM and verify /data/test-file.txt exists and has original content
+            12. SSH to VM and verify /data/modified.txt does NOT exist
+
+        Expected:
+            - File /data/test-file.txt contains original test data
+        """
+        vm = vm_with_rerun_on_failure
+        test_data = "original-test-data"
+        test_file = "/data/test-file.txt"
+        modified_file = "/data/modified.txt"
+
+        LOGGER.info(f"Step 1-2: Starting VM {vm.name} and writing test data")
+        vm.start(wait=True)
+        running_vm(vm=vm)
+
+        # Create directory and write test file
+        run_ssh_commands(host=vm.ssh_exec, commands=[shlex.split("mkdir -p /data")])
+        run_ssh_commands(
+            host=vm.ssh_exec,
+            commands=[shlex.split(f"echo '{test_data}' > {test_file}")],
+        )
+
+        LOGGER.info(f"Step 3-4: Creating snapshot and waiting for Ready state")
+        with VirtualMachineSnapshot(
+            name=f"{vm.name}-snapshot",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+        ) as snapshot:
+            snapshot.wait_ready_to_use(timeout=TIMEOUT_10MIN)
+
+            LOGGER.info(f"Step 5: Writing modified data after snapshot")
+            run_ssh_commands(
+                host=vm.ssh_exec,
+                commands=[shlex.split(f"echo 'modified-data' > {modified_file}")],
+            )
+
+            LOGGER.info(f"Step 6: Stopping VM {vm.name}")
+            vm.stop(wait=True)
+
+            LOGGER.info(f"Step 7-8: Creating restore and waiting for completion")
+            with VirtualMachineRestore(
+                name=f"{vm.name}-restore",
+                namespace=vm.namespace,
+                vm_name=vm.name,
+                snapshot_name=snapshot.name,
+            ) as vm_restore:
+                vm_restore.wait_restore_done(timeout=TIMEOUT_10MIN)
+
+        LOGGER.info(f"Step 9-10: Starting VM and waiting for Running state")
+        vm.start(wait=True)
+        running_vm(vm=vm)
+
+        LOGGER.info(f"Step 11: Verifying {test_file} has original content")
+        cmd = shlex.split(f"cat {test_file}")
+        result = run_ssh_commands(host=vm.ssh_exec, commands=cmd)[0]
+        assert result.strip() == test_data, (
+            f"File content mismatch: expected '{test_data}', got '{result.strip()}'"
+        )
+
+        LOGGER.info(f"Step 12: Verifying {modified_file} does not exist")
+        cmd = shlex.split(f"test -f {modified_file} && echo exists || echo not_exists")
+        result = run_ssh_commands(host=vm.ssh_exec, commands=cmd)[0]
+        assert result.strip() == "not_exists", (
+            f"File {modified_file} should not exist after restore"
+        )
+        LOGGER.info("Complete restore workflow validated successfully")
+
+
+class TestSnapshotRestoreRunStrategies:
+    """
+    Regression tests for snapshot restore with different run strategies.
+
+    These tests ensure the fix for RerunOnFailure doesn't break existing
+    functionality for other run strategies.
+    """
+
+    @pytest.mark.polarion("CNV-63819-08")
+    @pytest.mark.gating
+    @pytest.mark.parametrize(
+        "run_strategy",
+        [
+            pytest.param("Always", id="run_strategy_always"),
+            pytest.param("Manual", id="run_strategy_manual"),
+            pytest.param("Halted", id="run_strategy_halted"),
+            pytest.param("Once", id="run_strategy_once"),
+        ],
+    )
+    def test_restore_completes_with_various_run_strategies(
+        self,
+        vm_with_run_strategy_and_snapshot,
+        run_strategy,
+    ):
+        """
+        Test that snapshot restore completes for VMs with different run strategies.
+
+        This is a regression test to ensure the fix doesn't break other run strategies.
+
+        Steps:
+            1. Create VirtualMachineRestore resource
+            2. Wait for restore operation to complete
+
+        Expected:
+            - VirtualMachineRestore status is "Complete"
+        """
+        vm, snapshot = vm_with_run_strategy_and_snapshot
+        LOGGER.info(f"Testing restore for VM {vm.name} with runStrategy: {run_strategy}")
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as vm_restore:
+            LOGGER.info(f"Waiting for restore to complete for runStrategy: {run_strategy}")
+            vm_restore.wait_restore_done(timeout=TIMEOUT_5MIN)
+
+            assert vm_restore.instance.status.complete, (
+                f"VirtualMachineRestore failed for runStrategy {run_strategy}"
+            )
+            LOGGER.info(f"Restore completed successfully for runStrategy: {run_strategy}")
+
+    @pytest.mark.polarion("CNV-63819-09")
+    def test_restore_with_always_strategy(
+        self,
+        vm_with_always_strategy_stopped_with_snapshot,
+    ):
+        """
+        Test that snapshot restore completes for VM with runStrategy: Always.
+
+        Steps:
+            1. Create VirtualMachineRestore resource
+            2. Wait for restore operation to complete
+
+        Expected:
+            - VirtualMachineRestore status is "Complete"
+        """
+        vm, snapshot = vm_with_always_strategy_stopped_with_snapshot
+        LOGGER.info(f"Testing restore for VM {vm.name} with runStrategy: Always")
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as vm_restore:
+            vm_restore.wait_restore_done(timeout=TIMEOUT_5MIN)
+            assert vm_restore.instance.status.complete
+            LOGGER.info("Restore completed successfully for runStrategy: Always")
+
+    @pytest.mark.polarion("CNV-63819-10")
+    def test_restore_with_manual_strategy(
+        self,
+        vm_with_manual_strategy_stopped_with_snapshot,
+    ):
+        """
+        Test that snapshot restore completes for VM with runStrategy: Manual.
+
+        Steps:
+            1. Create VirtualMachineRestore resource
+            2. Wait for restore operation to complete
+
+        Expected:
+            - VirtualMachineRestore status is "Complete"
+        """
+        vm, snapshot = vm_with_manual_strategy_stopped_with_snapshot
+        LOGGER.info(f"Testing restore for VM {vm.name} with runStrategy: Manual")
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as vm_restore:
+            vm_restore.wait_restore_done(timeout=TIMEOUT_5MIN)
+            assert vm_restore.instance.status.complete
+            LOGGER.info("Restore completed successfully for runStrategy: Manual")
+
+    @pytest.mark.polarion("CNV-63819-11")
+    def test_restore_with_halted_strategy(
+        self,
+        vm_with_halted_strategy_with_snapshot,
+    ):
+        """
+        Test that snapshot restore completes for VM with runStrategy: Halted.
+
+        Steps:
+            1. Create VirtualMachineRestore resource
+            2. Wait for restore operation to complete
+
+        Expected:
+            - VirtualMachineRestore status is "Complete"
+        """
+        vm, snapshot = vm_with_halted_strategy_with_snapshot
+        LOGGER.info(f"Testing restore for VM {vm.name} with runStrategy: Halted")
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as vm_restore:
+            vm_restore.wait_restore_done(timeout=TIMEOUT_5MIN)
+            assert vm_restore.instance.status.complete
+            LOGGER.info("Restore completed successfully for runStrategy: Halted")
+
+
+class TestSnapshotRestoreMultipleOperations:
+    """
+    Tests for multiple snapshot and restore operations.
+    """
+
+    @pytest.mark.polarion("CNV-63819-12")
+    @pytest.mark.tier2
+    def test_restore_from_multiple_snapshots(
+        self,
+        vm_with_multiple_snapshots,
+    ):
+        """
+        Test that VM can be restored from different snapshots.
+
+        Steps:
+            1. Create VirtualMachineRestore from snap1
+            2. Wait for restore Complete
+            3. Start VM and verify /data/snapshot1.txt exists
+            4. Verify /data/snapshot2.txt does NOT exist
+            5. Stop VM
+            6. Create VirtualMachineRestore from snap2
+            7. Wait for restore Complete
+            8. Start VM and verify both files exist
+
+        Expected:
+            - Both /data/snapshot1.txt and /data/snapshot2.txt exist after second restore
+        """
+        vm, snap1, snap2 = vm_with_multiple_snapshots
+        file1 = "/data/snapshot1.txt"
+        file2 = "/data/snapshot2.txt"
+
+        LOGGER.info(f"Step 1-2: Restoring from first snapshot {snap1.name}")
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore-1",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snap1.name,
+        ) as restore1:
+            restore1.wait_restore_done(timeout=TIMEOUT_5MIN)
+
+        LOGGER.info("Step 3-4: Starting VM and verifying first snapshot state")
+        vm.start(wait=True)
+        running_vm(vm=vm)
+
+        # Verify file1 exists
+        cmd = shlex.split(f"test -f {file1} && echo exists || echo not_exists")
+        result = run_ssh_commands(host=vm.ssh_exec, commands=cmd)[0]
+        assert result.strip() == "exists", f"File {file1} should exist after first restore"
+
+        # Verify file2 does not exist
+        cmd = shlex.split(f"test -f {file2} && echo exists || echo not_exists")
+        result = run_ssh_commands(host=vm.ssh_exec, commands=cmd)[0]
+        assert result.strip() == "not_exists", f"File {file2} should not exist after first restore"
+
+        LOGGER.info("Step 5: Stopping VM")
+        vm.stop(wait=True)
+
+        LOGGER.info(f"Step 6-7: Restoring from second snapshot {snap2.name}")
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore-2",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snap2.name,
+        ) as restore2:
+            restore2.wait_restore_done(timeout=TIMEOUT_5MIN)
+
+        LOGGER.info("Step 8: Starting VM and verifying both files exist")
+        vm.start(wait=True)
+        running_vm(vm=vm)
+
+        # Verify both files exist
+        for test_file in [file1, file2]:
+            cmd = shlex.split(f"test -f {test_file} && echo exists || echo not_exists")
+            result = run_ssh_commands(host=vm.ssh_exec, commands=cmd)[0]
+            assert result.strip() == "exists", f"File {test_file} should exist after second restore"
+
+        LOGGER.info("Successfully restored from multiple snapshots")
+
+    @pytest.mark.polarion("CNV-63819-13")
+    @pytest.mark.tier2
+    def test_sequential_restore_operations(
+        self,
+        stopped_vm_with_snapshot,
+    ):
+        """
+        Test that multiple restore operations can be performed sequentially.
+
+        Steps:
+            1. Stop VM
+            2. Create VirtualMachineRestore (first restore)
+            3. Wait for restore Complete
+            4. Start VM, make changes, stop VM
+            5. Create VirtualMachineRestore (second restore from same snapshot)
+            6. Wait for restore Complete
+
+        Expected:
+            - Second VirtualMachineRestore status is "Complete"
+        """
+        vm, snapshot = stopped_vm_with_snapshot
+
+        LOGGER.info("Step 2-3: First restore operation")
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore-1",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as restore1:
+            restore1.wait_restore_done(timeout=TIMEOUT_5MIN)
+            assert restore1.instance.status.complete
+
+        LOGGER.info("Step 4: Start VM, make changes, stop VM")
+        vm.start(wait=True)
+        running_vm(vm=vm)
+        run_ssh_commands(
+            host=vm.ssh_exec,
+            commands=[shlex.split("echo 'change' > /tmp/modified.txt")],
+        )
+        vm.stop(wait=True)
+
+        LOGGER.info("Step 5-6: Second restore operation from same snapshot")
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore-2",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as restore2:
+            restore2.wait_restore_done(timeout=TIMEOUT_5MIN)
+            assert restore2.instance.status.complete
+            LOGGER.info("Sequential restore operations completed successfully")
+
+
+class TestSnapshotRestoreNegative:
+    """
+    Negative tests for snapshot restore error handling.
+    """
+
+    @pytest.mark.polarion("CNV-63819-14")
+    @pytest.mark.tier2
+    def test_restore_from_nonexistent_snapshot(
+        self,
+        vm_with_rerun_on_failure,
+    ):
+        """
+        [NEGATIVE] Test that restore fails gracefully when referencing non-existent snapshot.
+
+        Steps:
+            1. Create VirtualMachineRestore referencing snapshot name "does-not-exist"
+            2. Wait for restore to process
+            3. Check VirtualMachineRestore status
+
+        Expected:
+            - VirtualMachineRestore status is "Failed" or error condition set
+        """
+        vm = vm_with_rerun_on_failure
+        nonexistent_snapshot = "does-not-exist"
+
+        LOGGER.info(f"Creating restore with non-existent snapshot {nonexistent_snapshot}")
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore-fail",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=nonexistent_snapshot,
+            teardown=False,
+        ) as vm_restore:
+            # Wait for error condition
+            sampler = TimeoutSampler(
+                wait_timeout=TIMEOUT_5MIN,
+                sleep=5,
+                func=lambda: vm_restore.instance.status.get("conditions", []),
+            )
+
+            error_found = False
+            for conditions in sampler:
+                for condition in conditions:
+                    if condition.get("type") == "Ready" and condition.get("status") == "False":
+                        error_found = True
+                        LOGGER.info(
+                            f"Expected error condition found: {condition.get('message', '')}"
+                        )
+                        break
+                if error_found:
+                    break
+
+            assert error_found, "Expected error condition not found for non-existent snapshot"
+
+    @pytest.mark.polarion("CNV-63819-15")
+    @pytest.mark.tier2
+    def test_restore_while_vm_running(
+        self,
+        running_vm_with_snapshot,
+    ):
+        """
+        [NEGATIVE] Test that restore operation handles running VM appropriately.
+
+        Steps:
+            1. Attempt to create VirtualMachineRestore while VM is running
+            2. Check VirtualMachineRestore status
+
+        Expected:
+            - VirtualMachineRestore either fails or VM is automatically stopped first
+        """
+        vm, snapshot = running_vm_with_snapshot
+
+        LOGGER.info(f"Attempting to restore while VM {vm.name} is running")
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore-running",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as vm_restore:
+            # The restore should either fail or automatically stop the VM
+            # Check the status
+            sampler = TimeoutSampler(
+                wait_timeout=TIMEOUT_5MIN,
+                sleep=5,
+                func=lambda: vm_restore.instance.status,
+            )
+
+            for status in sampler:
+                if status.get("complete"):
+                    # Restore completed - VM must have been stopped automatically
+                    LOGGER.info("VM was automatically stopped and restore completed")
+                    break
+                elif status.get("conditions"):
+                    for condition in status["conditions"]:
+                        if condition.get("type") == "Ready" and condition.get("status") == "False":
+                            LOGGER.info(f"Restore failed as expected: {condition.get('message')}")
+                            return
+
+            # If we get here, restore completed (VM was auto-stopped)
+            # Check if VM is running - if not, restore succeeded by stopping VM
+            try:
+                vmi_check = VirtualMachineInstance(name=vm.name, namespace=vm.namespace)
+                assert vm_restore.instance.status.complete or not vmi_check.exists
+            except Exception:
+                # VMI doesn't exist - restore succeeded
+                pass
+
+
+class TestSnapshotRestoreMonitoring:
+    """
+    Tests for monitoring and observability during snapshot restore.
+    """
+
+    @pytest.mark.polarion("CNV-63819-16")
+    @pytest.mark.tier2
+    def test_restore_progress_monitoring(
+        self,
+        stopped_vm_with_snapshot,
+    ):
+        """
+        Test that restore progress can be monitored via VirtualMachineRestore status.
+
+        Steps:
+            1. Create VirtualMachineRestore
+            2. Poll VirtualMachineRestore status conditions
+            3. Monitor status progression from Progressing to Complete
+
+        Expected:
+            - VirtualMachineRestore transitions through expected status phases
+        """
+        vm, snapshot = stopped_vm_with_snapshot
+
+        LOGGER.info(f"Creating VirtualMachineRestore and monitoring status progression")
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore-monitor",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as vm_restore:
+            observed_states = set()
+
+            sampler = TimeoutSampler(
+                wait_timeout=TIMEOUT_5MIN,
+                sleep=2,
+                func=lambda: vm_restore.instance.status,
+            )
+
+            for status in sampler:
+                # Record conditions
+                conditions = status.get("conditions", [])
+                for condition in conditions:
+                    condition_type = condition.get("type")
+                    condition_status = condition.get("status")
+                    state = f"{condition_type}:{condition_status}"
+                    if state not in observed_states:
+                        observed_states.add(state)
+                        LOGGER.info(f"Observed state transition: {state}")
+
+                # Check for completion
+                if status.get("complete"):
+                    LOGGER.info("Restore completed")
+                    break
+
+            # Verify we observed expected states
+            assert len(observed_states) > 0, "No status conditions observed during restore"
+            LOGGER.info(f"Observed status transitions: {observed_states}")
+
+    @pytest.mark.polarion("CNV-63819-17")
+    @pytest.mark.tier2
+    def test_no_controller_errors_during_restore(
+        self,
+        stopped_vm_with_snapshot,
+        admin_client,
+    ):
+        """
+        Test that virt-controller logs no errors during restore operation.
+
+        Steps:
+            1. Create VirtualMachineRestore
+            2. Wait for restore Complete
+            3. Query virt-controller pod logs for ERROR level messages related to this VM
+
+        Expected:
+            - No ERROR messages in virt-controller logs for this VM during restore
+        """
+        vm, snapshot = stopped_vm_with_snapshot
+
+        LOGGER.info(f"Creating VirtualMachineRestore and monitoring virt-controller logs")
+
+        # Get virt-controller pod
+        from ocp_resources.pod import Pod
+        from utilities.constants import VIRT_CONTROLLER
+
+        virt_controller_pods = list(
+            Pod.get(
+                dyn_client=admin_client,
+                namespace="openshift-cnv",
+                label_selector=f"kubevirt.io={VIRT_CONTROLLER}",
+            )
+        )
+        assert virt_controller_pods, "No virt-controller pods found"
+        controller_pod = virt_controller_pods[0]
+
+        with VirtualMachineRestore(
+            name=f"{vm.name}-restore-logs",
+            namespace=vm.namespace,
+            vm_name=vm.name,
+            snapshot_name=snapshot.name,
+        ) as vm_restore:
+            vm_restore.wait_restore_done(timeout=TIMEOUT_5MIN)
+
+        # Check logs for errors related to this VM
+        LOGGER.info("Checking virt-controller logs for errors")
+        log_output = controller_pod.log(tail_lines=1000)
+
+        # Filter for lines containing VM name and ERROR level
+        error_lines = [
+            line
+            for line in log_output.split("\n")
+            if vm.name in line and ("ERROR" in line or "level=error" in line)
+        ]
+
+        assert not error_lines, (
+            f"Found ERROR messages in virt-controller logs for VM {vm.name}: {error_lines}"
+        )
+        LOGGER.info("No errors found in virt-controller logs during restore")
