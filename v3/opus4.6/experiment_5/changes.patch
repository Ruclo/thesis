diff --git a/GRAVEYARD.md b/GRAVEYARD.md
new file mode 100644
index 0000000..32d55d3
--- /dev/null
+++ b/GRAVEYARD.md
@@ -0,0 +1,289 @@
+# GRAVEYARD - Test Generation Lessons Learned
+
+This file documents mistakes made during automated test generation and how to avoid them.
+It is read during the exploration phase to prevent repeating past errors.
+
+---
+
+## Entry: Wrong HCO CR field name casing and value type for CommonInstancetypesDeployment
+
+**Category**: ResourceError
+**Date**: 2026-02-21
+**Test**: tests/install_upgrade_operators/common_instancetypes_deployment/conftest.py::disabled_common_instancetypes_deployment
+
+### What Went Wrong
+The generated code used `commonInstancetypesDeployment` (lowercase 'c') as the HCO CR spec field name with string values `"Enabled"` / `"Disabled"`. The actual CRD field is `CommonInstancetypesDeployment` (capital 'C') and its value is an object with a nested boolean `enabled` field, not a string.
+
+### Wrong Code
+```python
+COMMON_INSTANCETYPES_DEPLOYMENT_KEY = "commonInstancetypesDeployment"
+COMMON_INSTANCETYPES_DEPLOYMENT_ENABLED = "Enabled"
+COMMON_INSTANCETYPES_DEPLOYMENT_DISABLED = "Disabled"
+
+# Used in fixtures as:
+patches={
+    hyperconverged_resource: {
+        "spec": {COMMON_INSTANCETYPES_DEPLOYMENT_KEY: COMMON_INSTANCETYPES_DEPLOYMENT_DISABLED}
+    }
+}
+```
+
+### Correct Code
+```python
+COMMON_INSTANCETYPES_DEPLOYMENT_KEY = "CommonInstancetypesDeployment"
+COMMON_INSTANCETYPES_DEPLOYMENT_SPEC_ENABLED = {COMMON_INSTANCETYPES_DEPLOYMENT_KEY: {"enabled": True}}
+COMMON_INSTANCETYPES_DEPLOYMENT_SPEC_DISABLED = {COMMON_INSTANCETYPES_DEPLOYMENT_KEY: {"enabled": False}}
+
+# Used in fixtures as:
+patches={
+    hyperconverged_resource: {
+        "spec": COMMON_INSTANCETYPES_DEPLOYMENT_SPEC_DISABLED
+    }
+}
+```
+
+### Lesson Learned
+HCO CRD field names do not always follow standard camelCase conventions. Some fields like `CommonInstancetypesDeployment` start with a capital letter. Additionally, the field type may be an object with nested properties rather than a simple string enum. Always verify the actual CRD schema before generating code.
+
+### How to Avoid
+Before generating code that patches HCO CR spec fields, run `kubectl get crd hyperconvergeds.hco.kubevirt.io -o json` and inspect the `openAPIV3Schema` to confirm: (1) the exact field name casing, (2) the field type (string, boolean, object), and (3) nested property structure. Never assume field names follow standard camelCase or that toggle fields use string enum values.
+
+---
+
+## Entry: Wrong type hints on helper function parameters
+
+**Category**: TypeError
+**Date**: 2026-02-21
+**Test**: tests/install_upgrade_operators/common_instancetypes_deployment/conftest.py::get_common_cluster_instancetypes
+
+### What Went Wrong
+Helper functions that accept a Kubernetes `DynamicClient` parameter were annotated with incorrect types. The `admin_client` parameter was typed as `list[VirtualMachineClusterInstancetype]` (the return type was accidentally used as the parameter type) and `object` instead of `DynamicClient`.
+
+### Wrong Code
+```python
+def get_common_cluster_instancetypes(admin_client: list[VirtualMachineClusterInstancetype]) -> list[VirtualMachineClusterInstancetype]:
+    ...
+
+def wait_for_common_instancetypes_absent(admin_client: object, wait_timeout: int = TIMEOUT_5MIN) -> None:
+    ...
+```
+
+### Correct Code
+```python
+from kubernetes.dynamic import DynamicClient
+
+def get_common_cluster_instancetypes(admin_client: DynamicClient) -> list[VirtualMachineClusterInstancetype]:
+    ...
+
+def wait_for_common_instancetypes_absent(admin_client: DynamicClient, wait_timeout: int = TIMEOUT_5MIN) -> None:
+    ...
+```
+
+### Lesson Learned
+When writing type hints for Kubernetes client parameters, always use `DynamicClient` from `kubernetes.dynamic`. Do not confuse return types with parameter types, and do not use `object` as a generic stand-in.
+
+### How to Avoid
+For any function that takes a Kubernetes client parameter, always import and use `from kubernetes.dynamic import DynamicClient` as the type hint. Follow the pattern established in existing repository fixtures where `admin_client` is consistently typed as `DynamicClient`.
+
+---
+
+## Entry: Class-scoped fixtures from different VMs in same test class cause scheduling failures
+
+**Category**: ResourceError
+**Date**: 2026-02-21
+**Test**: tests/virt/screenshot/test_vm_screenshot.py::TestVMScreenshotGuestCompatibility::test_screenshot_with_rhel_guest
+
+### What Went Wrong
+Two class-scoped VM fixtures (`screenshot_fedora_vm` and `screenshot_rhel_vm`) were used by different tests within the same test class `TestVMScreenshotGuestCompatibility`. Because both fixtures were class-scoped, the Fedora VM remained alive for the entire class lifetime. When the RHEL test ran (still in the same class), the RHEL VM could not be scheduled because the cluster lacked sufficient memory to run both VMs simultaneously. The RHEL VM timed out with `ErrorUnschedulable: Insufficient memory`.
+
+### Wrong Code
+```python
+@pytest.mark.tier3
+class TestVMScreenshotGuestCompatibility:
+    def test_screenshot_with_fedora_guest(self, screenshot_fedora_vm, tmp_path):
+        ...
+
+    def test_screenshot_with_rhel_guest(self, screenshot_rhel_vm, tmp_path):
+        ...
+```
+
+### Correct Code
+```python
+@pytest.mark.tier3
+class TestVMScreenshotFedoraGuest:
+    def test_screenshot_with_fedora_guest(self, screenshot_fedora_vm, tmp_path):
+        ...
+
+
+@pytest.mark.tier3
+class TestVMScreenshotRhelGuest:
+    def test_screenshot_with_rhel_guest(self, screenshot_rhel_vm, tmp_path):
+        ...
+```
+
+### Lesson Learned
+When tests use different class-scoped VM fixtures, all fixtures remain alive for the entire class duration. On resource-constrained clusters, this means multiple VMs run simultaneously and can exceed available node memory. Each VM fixture that provisions a distinct VM should be in its own test class to ensure sequential teardown-then-create behavior.
+
+### How to Avoid
+Never put tests that use different class-scoped VM fixtures into the same test class. Each distinct VM fixture should be consumed by its own class so that one VM is torn down before the next is created. This prevents cluster scheduling failures due to insufficient resources.
+
+---
+
+## Entry: run_virtctl_command returns False when virtctl writes info logs to stderr
+
+**Category**: AssertionError
+**Date**: 2026-02-21
+**Test**: tests/virt/node/hard_reset/test_virtctl_reset.py::TestVirtctlReset::test_virtctl_reset_command_succeeds
+
+### What Went Wrong
+The `run_virtctl_command` function (which calls `run_command` from `pyhelper_utils.shell`) has `verify_stderr=True` by default. When `virtctl reset` succeeds, it writes an info-level JSON log message to stderr (e.g., `{"component":"portforward","level":"info","msg":"Reset VMI",...}`). Because stderr is non-empty and `verify_stderr=True`, `run_command` returns `(False, stdout, stderr)` even though the command exit code was 0.
+
+### Wrong Code
+```python
+command_success, output, error = run_virtctl_command(
+    command=["reset", vm.vmi.name],
+    namespace=vm.namespace,
+)
+assert command_success, f"virtctl reset command failed: {error}"
+```
+
+### Correct Code
+```python
+command_success, output, error = run_virtctl_command(
+    command=["reset", vm.vmi.name],
+    namespace=vm.namespace,
+    verify_stderr=False,
+)
+assert command_success, f"virtctl reset command failed: {error}"
+```
+
+### Lesson Learned
+Many virtctl commands write info-level log output to stderr even on success. The `run_virtctl_command` wrapper inherits `verify_stderr=True` from `run_command`, which treats any stderr output as a failure. For virtctl commands that are known to write logs to stderr, pass `verify_stderr=False` explicitly.
+
+### How to Avoid
+When using `run_virtctl_command`, always pass `verify_stderr=False` unless you specifically need to validate that stderr is empty. Virtctl subcommands commonly write structured JSON logs to stderr even on successful execution.
+
+---
+
+## Entry: ocp_resources retry_cluster_exceptions re-raises last_exp not TimeoutExpiredError
+
+**Category**: TypeError
+**Date**: 2026-02-21
+**Test**: tests/virt/node/hard_reset/test_vmi_reset_negative.py::TestVMIResetOnStoppedVM::test_reset_stopped_vmi
+
+### What Went Wrong
+The generated code expected `TimeoutExpiredError` to be raised when calling `vmi.reset()` on a non-existent or stopped VMI. However, `ocp_resources.resource.Resource.retry_cluster_exceptions` catches `TimeoutExpiredError` and re-raises `exp.last_exp` (the original `ApiException`) via `raise exp.last_exp from exp`. So the actual exception that propagates is `kubernetes.client.exceptions.ApiException(404)`, not `TimeoutExpiredError`.
+
+### Wrong Code
+```python
+from timeout_sampler import TimeoutExpiredError
+
+with pytest.raises(TimeoutExpiredError):
+    vmi.reset()
+```
+
+### Correct Code
+```python
+from kubernetes.client.exceptions import ApiException
+
+with pytest.raises(ApiException, match="404"):
+    vmi.reset()
+```
+
+### Lesson Learned
+When `ocp_resources` calls a Kubernetes API that returns a 404 (or other HTTP error), the retry mechanism in `retry_cluster_exceptions` catches the `TimeoutExpiredError` and re-raises the underlying `ApiException` with `raise exp.last_exp from exp`. The exception that reaches the test is always the raw `ApiException`, not `TimeoutExpiredError`.
+
+### How to Avoid
+When writing negative tests that expect a Kubernetes API error (404, 409, etc.) from `ocp_resources` methods like `reset()`, `pause()`, etc., always catch `kubernetes.client.exceptions.ApiException` with a `match` on the HTTP status code, NOT `TimeoutExpiredError` or `NotFoundError`. The `ocp_resources` retry layer unwraps the timeout and re-raises the raw API exception.
+
+---
+
+## Entry: Resetting a paused VMI succeeds without error
+
+**Category**: LogicError
+**Date**: 2026-02-21
+**Test**: tests/virt/node/hard_reset/test_vmi_reset_negative.py::TestVMIResetOnPausedVM::test_reset_paused_vmi
+
+### What Went Wrong
+The generated code assumed that calling `vmi.reset()` on a paused VMI would raise an error. In reality, the KubeVirt reset API accepts the reset call on a paused VMI and processes it successfully. The test was written as a negative test expecting `Exception` with match `"paused"`, but no exception was raised.
+
+### Wrong Code
+```python
+with pytest.raises(Exception, match="(paused|Paused)"):
+    paused_vm.vmi.reset()
+```
+
+### Correct Code
+```python
+# Reset on paused VMI succeeds - test as a positive case
+paused_vm.vmi.reset()
+```
+
+### Lesson Learned
+The KubeVirt hard reset API does not reject reset calls on paused VMIs. A hard reset is a hardware-level operation that works regardless of the guest pause state. Do not assume that pause state prevents reset operations.
+
+### How to Avoid
+Before writing negative tests that assume an API operation will fail on a paused VMI, verify the actual KubeVirt API behavior. Hard reset is a hardware signal that bypasses guest-level state. Only write negative reset tests for states where the VMI truly does not exist (stopped VM with no running VMI, non-existent VMI name).
+
+---
+
+## Entry: Unprivileged user (namespace admin) already has VMI reset permission
+
+**Category**: LogicError
+**Date**: 2026-02-21
+**Test**: tests/virt/node/hard_reset/test_vmi_reset_negative.py::TestVMIResetRBAC::test_unprivileged_user_cannot_reset_without_permission
+
+### What Went Wrong
+The generated code assumed that the unprivileged user (namespace admin) would NOT have permission to reset VMIs without an explicit `kubevirt.io:edit` RoleBinding. In reality, the namespace-admin role already includes the `virtualmachineinstances/reset` subresource permission. The test expected `ForbiddenError` but no exception was raised.
+
+### Wrong Code
+```python
+with pytest.raises(ForbiddenError):
+    rbac_vm.vmi.reset()
+```
+
+### Correct Code
+```python
+# Namespace admin already has reset permission - no negative RBAC test needed
+# Remove or convert to positive test
+rbac_vm.vmi.reset()
+```
+
+### Lesson Learned
+The KubeVirt RBAC model grants `virtualmachineinstances/reset` permission to namespace admin users. The `unprivileged_client` in this test framework is actually a namespace admin (it can create VMs, manage resources in its namespace). A truly unprivileged RBAC test would require creating a user with a more restricted role that explicitly lacks the reset subresource permission.
+
+### How to Avoid
+Before writing RBAC negative tests, verify what permissions the `unprivileged_client` fixture actually has. In this test framework, `unprivileged_client` is a namespace admin with broad permissions. For testing RBAC denial of new subresources, you need a custom ServiceAccount or User with a restricted ClusterRole that explicitly excludes the subresource being tested.
+
+---
+
+## Entry: SSH timeout too short after VMI hard reset
+
+**Category**: TimeoutError
+**Date**: 2026-02-21
+**Test**: tests/virt/node/hard_reset/conftest.py::hard_reset_vm_after_reset
+
+### What Went Wrong
+After performing a VMI hard reset, `wait_for_running_vm(vm=hard_reset_vm)` was called with the default `ssh_timeout=TIMEOUT_2MIN` (120 seconds). A hard reset is more disruptive than a normal reboot - the guest OS must cold-boot from scratch, which can take longer for SSH to become available. The SSH connection timed out with `SSHException: Error reading SSH protocol banner` after 120 seconds.
+
+### Wrong Code
+```python
+hard_reset_vm.vmi.reset()
+wait_for_running_vm(vm=hard_reset_vm)
+```
+
+### Correct Code
+```python
+from utilities.constants import TIMEOUT_10MIN
+
+hard_reset_vm.vmi.reset()
+wait_for_running_vm(vm=hard_reset_vm, ssh_timeout=TIMEOUT_10MIN)
+```
+
+### Lesson Learned
+A VMI hard reset simulates a hardware reset button press, causing a full cold boot of the guest OS. SSH readiness after a hard reset takes significantly longer than after a soft reboot because the guest must complete the entire boot sequence from BIOS/UEFI. The default `ssh_timeout=TIMEOUT_2MIN` is insufficient. Use `TIMEOUT_10MIN` to allow adequate time for the cold boot + SSH daemon startup.
+
+### How to Avoid
+When waiting for a VM to become SSH-accessible after a hard reset operation, always pass an increased `ssh_timeout` (at least `TIMEOUT_10MIN`) to `wait_for_running_vm`. The default 2-minute timeout is only appropriate for warm reboots where the guest OS restarts quickly.
+
+---
diff --git a/tests/std/storageprofile_snapshot_class/std_cnv_61266.md b/tests/std/storageprofile_snapshot_class/std_cnv_61266.md
new file mode 100644
index 0000000..e3db32b
--- /dev/null
+++ b/tests/std/storageprofile_snapshot_class/std_cnv_61266.md
@@ -0,0 +1,156 @@
+# Software Test Description (STD)
+
+## Feature: StorageProfile snapshotClass Honored for VM Snapshot
+
+**STP Reference:** [/home/fedora/thesis/stps/5.md](/home/fedora/thesis/stps/5.md)
+**Jira ID:** [CNV-61266](https://issues.redhat.com/browse/CNV-61266) / [CNV-54866](https://issues.redhat.com/browse/CNV-54866)
+**Generated:** 2026-02-21
+
+---
+
+## Summary
+
+This STD covers tests verifying that the VMSnapshot controller honors the `snapshotClass` field from `StorageProfile` when selecting a `VolumeSnapshotClass` for VM snapshots. Tests cover the primary behavior (snapshotClass honored), fallback behavior (label-based selection when no snapshotClass is set), and snapshot restore using the correct VolumeSnapshotClass.
+
+---
+
+## Test Files
+
+### File: `tests/storage/snapshots/test_storageprofile_snapshot_class.py`
+
+```python
+"""
+StorageProfile snapshotClass Honored for VM Snapshot Tests
+
+STP Reference: /home/fedora/thesis/stps/5.md
+
+This module contains tests verifying that VMSnapshot honors the snapshotClass
+field from StorageProfile when selecting a VolumeSnapshotClass. Covers both
+the primary path (snapshotClass set) and the fallback path (label-based
+selection), as well as snapshot restore with the correct VolumeSnapshotClass.
+"""
+
+import pytest
+
+
+class TestStorageProfileSnapshotClass:
+    """
+    Tests for StorageProfile snapshotClass selection during VM snapshot creation.
+
+    Markers:
+        - storage
+
+    Preconditions:
+        - Cluster with snapshot-capable storage backend (e.g., ODF/Ceph)
+        - At least one VolumeSnapshotClass available for the storage provisioner
+        - Running VM with a data volume backed by a snapshot-capable StorageClass
+    """
+
+    def test_snapshot_uses_storageprofile_snapshot_class(self):
+        """
+        Test that VMSnapshot uses the snapshotClass specified in the StorageProfile.
+
+        Preconditions:
+            - StorageProfile for the VM's StorageClass has snapshotClass set
+              to a specific VolumeSnapshotClass name
+
+        Steps:
+            1. Create a VMSnapshot of the running VM
+            2. Wait for VMSnapshot to become ready
+            3. Retrieve the VolumeSnapshot created by the VMSnapshot
+            4. Read the volumeSnapshotClassName from the VolumeSnapshot spec
+
+        Expected:
+            - VolumeSnapshot's volumeSnapshotClassName equals the snapshotClass
+              value from the StorageProfile
+        """
+        pass
+
+    def test_snapshot_falls_back_to_label_based_selection(self):
+        """
+        Test that VMSnapshot falls back to label-based VolumeSnapshotClass
+        selection when StorageProfile has no snapshotClass set.
+
+        Preconditions:
+            - StorageProfile for the VM's StorageClass does NOT have
+              snapshotClass set (field is empty or absent)
+
+        Steps:
+            1. Create a VMSnapshot of the running VM
+            2. Wait for VMSnapshot to become ready
+            3. Retrieve the VolumeSnapshot created by the VMSnapshot
+            4. Read the volumeSnapshotClassName from the VolumeSnapshot spec
+
+        Expected:
+            - VolumeSnapshot's volumeSnapshotClassName equals the
+              VolumeSnapshotClass whose driver matches the StorageClass
+              provisioner (label-based fallback selection)
+        """
+        pass
+
+    def test_restore_vm_from_snapshot_with_storageprofile_snapshot_class(self):
+        """
+        Test that a VM can be successfully restored from a snapshot created
+        using the StorageProfile-specified snapshotClass.
+
+        Preconditions:
+            - StorageProfile for the VM's StorageClass has snapshotClass set
+            - VMSnapshot created and ready (using StorageProfile snapshotClass)
+            - VM is stopped
+
+        Steps:
+            1. Create a VirtualMachineRestore from the VMSnapshot
+            2. Wait for restore to complete
+            3. Start the VM and wait for it to become running and SSH accessible
+
+        Expected:
+            - VM is "Running" and SSH accessible after restore
+        """
+        pass
+```
+
+---
+
+## Test Coverage Summary
+
+| Test File | Test Class | Test Count | Priority | Tier |
+| --- | --- | --- | --- | --- |
+| `test_storageprofile_snapshot_class.py` | `TestStorageProfileSnapshotClass` | 3 | P0, P0, P1 | Tier 1 |
+
+---
+
+## Traceability Matrix
+
+| Requirement ID | Requirement Summary | Test Method | Priority |
+| --- | --- | --- | --- |
+| CNV-61266 | Honor snapshotClass from StorageProfile | `test_snapshot_uses_storageprofile_snapshot_class` | P0 |
+| CNV-61266 | Fallback to label-based selection | `test_snapshot_falls_back_to_label_based_selection` | P0 |
+| CNV-61266 | Restore with correct VolumeSnapshotClass | `test_restore_vm_from_snapshot_with_storageprofile_snapshot_class` | P1 |
+
+---
+
+## Design Decisions
+
+1. **Scenario 4 (Multiple Storage Classes) excluded from Tier 1**: The STP lists this as Tier 2 / P2. The three Tier 1 scenarios fully cover the core fix (honor snapshotClass, fallback, restore). Multi-StorageClass coverage can be added separately if needed.
+
+2. **Single test class**: All three tests share the same preconditions (snapshot-capable cluster, running VM) and are closely related, so they are grouped under one class.
+
+3. **StorageProfile modification as fixture concern**: The snapshotClass configuration on StorageProfile is a precondition, not a test step. The implementation phase will use a fixture (with `ResourceEditor` or equivalent) to set/unset the snapshotClass field, keeping test bodies focused on verification.
+
+4. **Fallback verification approach**: The fallback test verifies that the selected VolumeSnapshotClass matches the one whose driver corresponds to the StorageClass provisioner, which is the label-based selection mechanism.
+
+---
+
+## Checklist
+
+- [x] STP link in module docstring
+- [x] Tests grouped in class with shared preconditions
+- [x] Each test has: description, Preconditions, Steps, Expected
+- [x] Each test verifies ONE thing with ONE Expected
+- [x] Negative tests marked with `[NEGATIVE]` (none applicable)
+- [x] Test methods contain only `pass`
+- [x] Markers documented (storage)
+- [x] Parametrization documented where needed (none needed for Tier 1)
+- [x] All files in single markdown output
+- [x] Coverage summary table included
+- [x] Output saved to `tests/std/storageprofile_snapshot_class/std_cnv_61266.md`
diff --git a/tests/storage/snapshots/conftest.py b/tests/storage/snapshots/conftest.py
index 4db9193..01aa45e 100644
--- a/tests/storage/snapshots/conftest.py
+++ b/tests/storage/snapshots/conftest.py
@@ -9,8 +9,12 @@ import shlex
 
 import pytest
 from ocp_resources.datavolume import DataVolume
+from ocp_resources.resource import ResourceEditor
 from ocp_resources.role_binding import RoleBinding
+from ocp_resources.storage_class import StorageClass
+from ocp_resources.storage_profile import StorageProfile
 from ocp_resources.virtual_machine_snapshot import VirtualMachineSnapshot
+from ocp_resources.volume_snapshot_class import VolumeSnapshotClass
 from pyhelper_utils.shell import run_ssh_commands
 
 from tests.storage.snapshots.constants import WINDOWS_DIRECTORY_PATH
@@ -106,3 +110,45 @@ def file_created_during_snapshot(windows_vm_for_snapshot, windows_snapshot):
     run_ssh_commands(host=windows_vm_for_snapshot.ssh_exec, commands=cmd)
     windows_snapshot.wait_snapshot_done(timeout=TIMEOUT_10MIN)
     windows_vm_for_snapshot.stop(wait=True)
+
+
+@pytest.fixture()
+def storageprofile_with_snapshot_class(
+    admin_client,
+    snapshot_storage_class_name_scope_module,
+):
+    """StorageProfile patched with snapshotClass set to the matching VolumeSnapshotClass.
+
+    Finds the VolumeSnapshotClass whose driver matches the StorageClass provisioner,
+    then patches the StorageProfile spec.snapshotClass to that VolumeSnapshotClass name.
+    Restores the original StorageProfile on teardown via ResourceEditor context manager.
+    """
+    storage_class_instance = StorageClass(
+        client=admin_client,
+        name=snapshot_storage_class_name_scope_module,
+    ).instance
+    provisioner = storage_class_instance.provisioner
+
+    matching_vsc_name = None
+    for volume_snapshot_class in VolumeSnapshotClass.get(client=admin_client):
+        if volume_snapshot_class.instance.driver == provisioner:
+            matching_vsc_name = volume_snapshot_class.name
+            break
+
+    if not matching_vsc_name:
+        pytest.skip(
+            f"No VolumeSnapshotClass found with driver matching provisioner '{provisioner}'"
+        )
+
+    storage_profile = StorageProfile(
+        name=snapshot_storage_class_name_scope_module,
+        client=admin_client,
+    )
+    LOGGER.info(
+        f"Patching StorageProfile '{storage_profile.name}' with "
+        f"snapshotClass='{matching_vsc_name}'"
+    )
+    with ResourceEditor(
+        patches={storage_profile: {"spec": {"snapshotClass": matching_vsc_name}}}
+    ):
+        yield storage_profile
diff --git a/tests/storage/snapshots/test_storageprofile_snapshot_class.py b/tests/storage/snapshots/test_storageprofile_snapshot_class.py
new file mode 100644
index 0000000..c88d1ce
--- /dev/null
+++ b/tests/storage/snapshots/test_storageprofile_snapshot_class.py
@@ -0,0 +1,273 @@
+# -*- coding: utf-8 -*-
+
+"""
+StorageProfile snapshotClass Honored for VM Snapshot Tests
+
+STP Reference: /home/fedora/thesis/stps/5.md
+
+This module contains tests verifying that VMSnapshot honors the snapshotClass
+field from StorageProfile when selecting a VolumeSnapshotClass. Covers both
+the primary path (snapshotClass set) and the fallback path (label-based
+selection), as well as snapshot restore with the correct VolumeSnapshotClass.
+"""
+
+import logging
+
+import pytest
+from kubernetes.dynamic import DynamicClient
+from ocp_resources.storage_class import StorageClass
+from ocp_resources.virtual_machine_restore import VirtualMachineRestore
+from ocp_resources.virtual_machine_snapshot import VirtualMachineSnapshot
+from ocp_resources.volume_snapshot import VolumeSnapshot
+from ocp_resources.volume_snapshot_class import VolumeSnapshotClass
+
+from utilities.constants import TIMEOUT_8MIN
+from utilities.virt import running_vm
+
+LOGGER = logging.getLogger(__name__)
+
+pytestmark = pytest.mark.usefixtures(
+    "skip_if_no_storage_class_for_snapshot",
+)
+
+
+def get_volume_snapshot_class_for_sc_provisioner(
+    storage_class_name: str,
+    admin_client: DynamicClient,
+) -> str:
+    """Find the VolumeSnapshotClass whose driver matches the StorageClass provisioner.
+
+    Args:
+        storage_class_name: Name of the StorageClass to match.
+        admin_client: Kubernetes dynamic client.
+
+    Returns:
+        Name of the matching VolumeSnapshotClass.
+    """
+    storage_class_instance = StorageClass(client=admin_client, name=storage_class_name).instance
+    provisioner = storage_class_instance.provisioner
+    for volume_snapshot_class in VolumeSnapshotClass.get(client=admin_client):
+        if volume_snapshot_class.instance.driver == provisioner:
+            return volume_snapshot_class.name
+    raise ValueError(f"No VolumeSnapshotClass found with driver matching provisioner '{provisioner}'")
+
+
+def get_volume_snapshot_class_name_from_vm_snapshot(
+    vm_snapshot: VirtualMachineSnapshot,
+    admin_client: DynamicClient,
+) -> str:
+    """Retrieve the volumeSnapshotClassName from the VolumeSnapshot created by a VMSnapshot.
+
+    The VirtualMachineSnapshot status contains a virtualMachineSnapshotContentName
+    that references a VirtualMachineSnapshotContent. That content has volumeBackups
+    with volumeSnapshotName entries pointing to the actual VolumeSnapshot objects.
+
+    Args:
+        vm_snapshot: A ready VirtualMachineSnapshot.
+        admin_client: Kubernetes dynamic client.
+
+    Returns:
+        The volumeSnapshotClassName from the first VolumeSnapshot.
+    """
+    snapshot_content_name = vm_snapshot.instance.status.virtualMachineSnapshotContentName
+    LOGGER.info(f"VMSnapshot content name: {snapshot_content_name}")
+
+    volume_snapshots = list(VolumeSnapshot.get(
+        client=admin_client,
+        namespace=vm_snapshot.namespace,
+    ))
+    LOGGER.info(f"Found {len(volume_snapshots)} VolumeSnapshots in namespace {vm_snapshot.namespace}")
+
+    for volume_snapshot in volume_snapshots:
+        volume_snapshot_class_name = volume_snapshot.instance.spec.get("volumeSnapshotClassName")
+        if volume_snapshot_class_name:
+            LOGGER.info(
+                f"VolumeSnapshot '{volume_snapshot.name}' uses "
+                f"volumeSnapshotClassName: '{volume_snapshot_class_name}'"
+            )
+            return volume_snapshot_class_name
+
+    raise ValueError(
+        f"No VolumeSnapshot with volumeSnapshotClassName found in namespace '{vm_snapshot.namespace}'"
+    )
+
+
+class TestStorageProfileSnapshotClassHonored:
+    """
+    Tests for StorageProfile snapshotClass selection during VM snapshot creation.
+
+    Preconditions:
+        - Cluster with snapshot-capable storage backend (e.g., ODF/Ceph)
+        - At least one VolumeSnapshotClass available for the storage provisioner
+        - Running VM with a data volume backed by a snapshot-capable StorageClass
+        - StorageProfile for the VM's StorageClass has snapshotClass set
+          to a specific VolumeSnapshotClass name
+    """
+
+    @pytest.mark.parametrize(
+        "rhel_vm_name",
+        [
+            pytest.param(
+                {"vm_name": "vm-cnv-61266-sp-honored"},
+                marks=pytest.mark.polarion("CNV-61266"),
+            ),
+        ],
+        indirect=True,
+    )
+    def test_snapshot_uses_storageprofile_snapshot_class(
+        self,
+        admin_client,
+        rhel_vm_for_snapshot,
+        snapshot_storage_class_name_scope_module,
+        storageprofile_with_snapshot_class,
+    ):
+        """
+        Test that VMSnapshot uses the snapshotClass specified in the StorageProfile.
+
+        Steps:
+            1. Create a VMSnapshot of the running VM
+            2. Wait for VMSnapshot to become ready
+            3. Retrieve the VolumeSnapshot created by the VMSnapshot
+            4. Read the volumeSnapshotClassName from the VolumeSnapshot spec
+
+        Expected:
+            - VolumeSnapshot's volumeSnapshotClassName equals the snapshotClass
+              value from the StorageProfile
+        """
+        expected_snapshot_class = storageprofile_with_snapshot_class.snapshotclass
+        LOGGER.info(f"Expected snapshotClass from StorageProfile: {expected_snapshot_class}")
+
+        with VirtualMachineSnapshot(
+            name="vm-snapshot-sp-honored",
+            namespace=rhel_vm_for_snapshot.namespace,
+            vm_name=rhel_vm_for_snapshot.name,
+            client=admin_client,
+        ) as vm_snapshot:
+            vm_snapshot.wait_snapshot_done(timeout=TIMEOUT_8MIN)
+
+            actual_snapshot_class = get_volume_snapshot_class_name_from_vm_snapshot(
+                vm_snapshot=vm_snapshot,
+                admin_client=admin_client,
+            )
+            assert actual_snapshot_class == expected_snapshot_class, (
+                f"VolumeSnapshot used '{actual_snapshot_class}' but StorageProfile "
+                f"specified '{expected_snapshot_class}'"
+            )
+
+    @pytest.mark.parametrize(
+        "rhel_vm_name",
+        [
+            pytest.param(
+                {"vm_name": "vm-cnv-61266-sp-restore"},
+                marks=pytest.mark.polarion("CNV-61266"),
+            ),
+        ],
+        indirect=True,
+    )
+    def test_restore_vm_from_snapshot_with_storageprofile_snapshot_class(
+        self,
+        admin_client,
+        rhel_vm_for_snapshot,
+        storageprofile_with_snapshot_class,
+    ):
+        """
+        Test that a VM can be successfully restored from a snapshot created
+        using the StorageProfile-specified snapshotClass.
+
+        Steps:
+            1. Create a VMSnapshot and wait for it to become ready
+            2. Stop the VM
+            3. Create a VirtualMachineRestore from the VMSnapshot
+            4. Wait for restore to complete
+            5. Start the VM and wait for it to become running and SSH accessible
+
+        Expected:
+            - VM is "Running" and SSH accessible after restore
+        """
+        with VirtualMachineSnapshot(
+            name="vm-snapshot-sp-restore",
+            namespace=rhel_vm_for_snapshot.namespace,
+            vm_name=rhel_vm_for_snapshot.name,
+            client=admin_client,
+        ) as vm_snapshot:
+            vm_snapshot.wait_snapshot_done(timeout=TIMEOUT_8MIN)
+
+            rhel_vm_for_snapshot.stop(wait=True)
+
+            with VirtualMachineRestore(
+                name="vm-restore-sp-snapshot-class",
+                namespace=rhel_vm_for_snapshot.namespace,
+                vm_name=rhel_vm_for_snapshot.name,
+                snapshot_name=vm_snapshot.name,
+                client=admin_client,
+            ) as vm_restore:
+                vm_restore.wait_restore_done()
+
+                running_vm(vm=rhel_vm_for_snapshot)
+                LOGGER.info("VM is running and SSH accessible after restore")
+
+
+class TestStorageProfileSnapshotClassFallback:
+    """
+    Tests for StorageProfile snapshotClass fallback behavior.
+
+    Preconditions:
+        - Cluster with snapshot-capable storage backend (e.g., ODF/Ceph)
+        - At least one VolumeSnapshotClass available for the storage provisioner
+        - Running VM with a data volume backed by a snapshot-capable StorageClass
+        - StorageProfile for the VM's StorageClass does NOT have snapshotClass set
+    """
+
+    @pytest.mark.parametrize(
+        "rhel_vm_name",
+        [
+            pytest.param(
+                {"vm_name": "vm-cnv-61266-fallback"},
+                marks=pytest.mark.polarion("CNV-61266"),
+            ),
+        ],
+        indirect=True,
+    )
+    def test_snapshot_falls_back_to_label_based_selection(
+        self,
+        admin_client,
+        rhel_vm_for_snapshot,
+        snapshot_storage_class_name_scope_module,
+    ):
+        """
+        Test that VMSnapshot falls back to label-based VolumeSnapshotClass
+        selection when StorageProfile has no snapshotClass set.
+
+        Steps:
+            1. Create a VMSnapshot of the running VM
+            2. Wait for VMSnapshot to become ready
+            3. Retrieve the VolumeSnapshot created by the VMSnapshot
+            4. Read the volumeSnapshotClassName from the VolumeSnapshot spec
+
+        Expected:
+            - VolumeSnapshot's volumeSnapshotClassName equals the
+              VolumeSnapshotClass whose driver matches the StorageClass
+              provisioner (label-based fallback selection)
+        """
+        expected_snapshot_class = get_volume_snapshot_class_for_sc_provisioner(
+            storage_class_name=snapshot_storage_class_name_scope_module,
+            admin_client=admin_client,
+        )
+        LOGGER.info(f"Expected fallback VolumeSnapshotClass (by driver match): {expected_snapshot_class}")
+
+        with VirtualMachineSnapshot(
+            name="vm-snapshot-fallback",
+            namespace=rhel_vm_for_snapshot.namespace,
+            vm_name=rhel_vm_for_snapshot.name,
+            client=admin_client,
+        ) as vm_snapshot:
+            vm_snapshot.wait_snapshot_done(timeout=TIMEOUT_8MIN)
+
+            actual_snapshot_class = get_volume_snapshot_class_name_from_vm_snapshot(
+                vm_snapshot=vm_snapshot,
+                admin_client=admin_client,
+            )
+            assert actual_snapshot_class == expected_snapshot_class, (
+                f"VolumeSnapshot used '{actual_snapshot_class}' but expected fallback "
+                f"VolumeSnapshotClass '{expected_snapshot_class}' (driver-based selection)"
+            )
