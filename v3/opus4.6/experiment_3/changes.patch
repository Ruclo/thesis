diff --git a/GRAVEYARD.md b/GRAVEYARD.md
new file mode 100644
index 0000000..32d55d3
--- /dev/null
+++ b/GRAVEYARD.md
@@ -0,0 +1,289 @@
+# GRAVEYARD - Test Generation Lessons Learned
+
+This file documents mistakes made during automated test generation and how to avoid them.
+It is read during the exploration phase to prevent repeating past errors.
+
+---
+
+## Entry: Wrong HCO CR field name casing and value type for CommonInstancetypesDeployment
+
+**Category**: ResourceError
+**Date**: 2026-02-21
+**Test**: tests/install_upgrade_operators/common_instancetypes_deployment/conftest.py::disabled_common_instancetypes_deployment
+
+### What Went Wrong
+The generated code used `commonInstancetypesDeployment` (lowercase 'c') as the HCO CR spec field name with string values `"Enabled"` / `"Disabled"`. The actual CRD field is `CommonInstancetypesDeployment` (capital 'C') and its value is an object with a nested boolean `enabled` field, not a string.
+
+### Wrong Code
+```python
+COMMON_INSTANCETYPES_DEPLOYMENT_KEY = "commonInstancetypesDeployment"
+COMMON_INSTANCETYPES_DEPLOYMENT_ENABLED = "Enabled"
+COMMON_INSTANCETYPES_DEPLOYMENT_DISABLED = "Disabled"
+
+# Used in fixtures as:
+patches={
+    hyperconverged_resource: {
+        "spec": {COMMON_INSTANCETYPES_DEPLOYMENT_KEY: COMMON_INSTANCETYPES_DEPLOYMENT_DISABLED}
+    }
+}
+```
+
+### Correct Code
+```python
+COMMON_INSTANCETYPES_DEPLOYMENT_KEY = "CommonInstancetypesDeployment"
+COMMON_INSTANCETYPES_DEPLOYMENT_SPEC_ENABLED = {COMMON_INSTANCETYPES_DEPLOYMENT_KEY: {"enabled": True}}
+COMMON_INSTANCETYPES_DEPLOYMENT_SPEC_DISABLED = {COMMON_INSTANCETYPES_DEPLOYMENT_KEY: {"enabled": False}}
+
+# Used in fixtures as:
+patches={
+    hyperconverged_resource: {
+        "spec": COMMON_INSTANCETYPES_DEPLOYMENT_SPEC_DISABLED
+    }
+}
+```
+
+### Lesson Learned
+HCO CRD field names do not always follow standard camelCase conventions. Some fields like `CommonInstancetypesDeployment` start with a capital letter. Additionally, the field type may be an object with nested properties rather than a simple string enum. Always verify the actual CRD schema before generating code.
+
+### How to Avoid
+Before generating code that patches HCO CR spec fields, run `kubectl get crd hyperconvergeds.hco.kubevirt.io -o json` and inspect the `openAPIV3Schema` to confirm: (1) the exact field name casing, (2) the field type (string, boolean, object), and (3) nested property structure. Never assume field names follow standard camelCase or that toggle fields use string enum values.
+
+---
+
+## Entry: Wrong type hints on helper function parameters
+
+**Category**: TypeError
+**Date**: 2026-02-21
+**Test**: tests/install_upgrade_operators/common_instancetypes_deployment/conftest.py::get_common_cluster_instancetypes
+
+### What Went Wrong
+Helper functions that accept a Kubernetes `DynamicClient` parameter were annotated with incorrect types. The `admin_client` parameter was typed as `list[VirtualMachineClusterInstancetype]` (the return type was accidentally used as the parameter type) and `object` instead of `DynamicClient`.
+
+### Wrong Code
+```python
+def get_common_cluster_instancetypes(admin_client: list[VirtualMachineClusterInstancetype]) -> list[VirtualMachineClusterInstancetype]:
+    ...
+
+def wait_for_common_instancetypes_absent(admin_client: object, wait_timeout: int = TIMEOUT_5MIN) -> None:
+    ...
+```
+
+### Correct Code
+```python
+from kubernetes.dynamic import DynamicClient
+
+def get_common_cluster_instancetypes(admin_client: DynamicClient) -> list[VirtualMachineClusterInstancetype]:
+    ...
+
+def wait_for_common_instancetypes_absent(admin_client: DynamicClient, wait_timeout: int = TIMEOUT_5MIN) -> None:
+    ...
+```
+
+### Lesson Learned
+When writing type hints for Kubernetes client parameters, always use `DynamicClient` from `kubernetes.dynamic`. Do not confuse return types with parameter types, and do not use `object` as a generic stand-in.
+
+### How to Avoid
+For any function that takes a Kubernetes client parameter, always import and use `from kubernetes.dynamic import DynamicClient` as the type hint. Follow the pattern established in existing repository fixtures where `admin_client` is consistently typed as `DynamicClient`.
+
+---
+
+## Entry: Class-scoped fixtures from different VMs in same test class cause scheduling failures
+
+**Category**: ResourceError
+**Date**: 2026-02-21
+**Test**: tests/virt/screenshot/test_vm_screenshot.py::TestVMScreenshotGuestCompatibility::test_screenshot_with_rhel_guest
+
+### What Went Wrong
+Two class-scoped VM fixtures (`screenshot_fedora_vm` and `screenshot_rhel_vm`) were used by different tests within the same test class `TestVMScreenshotGuestCompatibility`. Because both fixtures were class-scoped, the Fedora VM remained alive for the entire class lifetime. When the RHEL test ran (still in the same class), the RHEL VM could not be scheduled because the cluster lacked sufficient memory to run both VMs simultaneously. The RHEL VM timed out with `ErrorUnschedulable: Insufficient memory`.
+
+### Wrong Code
+```python
+@pytest.mark.tier3
+class TestVMScreenshotGuestCompatibility:
+    def test_screenshot_with_fedora_guest(self, screenshot_fedora_vm, tmp_path):
+        ...
+
+    def test_screenshot_with_rhel_guest(self, screenshot_rhel_vm, tmp_path):
+        ...
+```
+
+### Correct Code
+```python
+@pytest.mark.tier3
+class TestVMScreenshotFedoraGuest:
+    def test_screenshot_with_fedora_guest(self, screenshot_fedora_vm, tmp_path):
+        ...
+
+
+@pytest.mark.tier3
+class TestVMScreenshotRhelGuest:
+    def test_screenshot_with_rhel_guest(self, screenshot_rhel_vm, tmp_path):
+        ...
+```
+
+### Lesson Learned
+When tests use different class-scoped VM fixtures, all fixtures remain alive for the entire class duration. On resource-constrained clusters, this means multiple VMs run simultaneously and can exceed available node memory. Each VM fixture that provisions a distinct VM should be in its own test class to ensure sequential teardown-then-create behavior.
+
+### How to Avoid
+Never put tests that use different class-scoped VM fixtures into the same test class. Each distinct VM fixture should be consumed by its own class so that one VM is torn down before the next is created. This prevents cluster scheduling failures due to insufficient resources.
+
+---
+
+## Entry: run_virtctl_command returns False when virtctl writes info logs to stderr
+
+**Category**: AssertionError
+**Date**: 2026-02-21
+**Test**: tests/virt/node/hard_reset/test_virtctl_reset.py::TestVirtctlReset::test_virtctl_reset_command_succeeds
+
+### What Went Wrong
+The `run_virtctl_command` function (which calls `run_command` from `pyhelper_utils.shell`) has `verify_stderr=True` by default. When `virtctl reset` succeeds, it writes an info-level JSON log message to stderr (e.g., `{"component":"portforward","level":"info","msg":"Reset VMI",...}`). Because stderr is non-empty and `verify_stderr=True`, `run_command` returns `(False, stdout, stderr)` even though the command exit code was 0.
+
+### Wrong Code
+```python
+command_success, output, error = run_virtctl_command(
+    command=["reset", vm.vmi.name],
+    namespace=vm.namespace,
+)
+assert command_success, f"virtctl reset command failed: {error}"
+```
+
+### Correct Code
+```python
+command_success, output, error = run_virtctl_command(
+    command=["reset", vm.vmi.name],
+    namespace=vm.namespace,
+    verify_stderr=False,
+)
+assert command_success, f"virtctl reset command failed: {error}"
+```
+
+### Lesson Learned
+Many virtctl commands write info-level log output to stderr even on success. The `run_virtctl_command` wrapper inherits `verify_stderr=True` from `run_command`, which treats any stderr output as a failure. For virtctl commands that are known to write logs to stderr, pass `verify_stderr=False` explicitly.
+
+### How to Avoid
+When using `run_virtctl_command`, always pass `verify_stderr=False` unless you specifically need to validate that stderr is empty. Virtctl subcommands commonly write structured JSON logs to stderr even on successful execution.
+
+---
+
+## Entry: ocp_resources retry_cluster_exceptions re-raises last_exp not TimeoutExpiredError
+
+**Category**: TypeError
+**Date**: 2026-02-21
+**Test**: tests/virt/node/hard_reset/test_vmi_reset_negative.py::TestVMIResetOnStoppedVM::test_reset_stopped_vmi
+
+### What Went Wrong
+The generated code expected `TimeoutExpiredError` to be raised when calling `vmi.reset()` on a non-existent or stopped VMI. However, `ocp_resources.resource.Resource.retry_cluster_exceptions` catches `TimeoutExpiredError` and re-raises `exp.last_exp` (the original `ApiException`) via `raise exp.last_exp from exp`. So the actual exception that propagates is `kubernetes.client.exceptions.ApiException(404)`, not `TimeoutExpiredError`.
+
+### Wrong Code
+```python
+from timeout_sampler import TimeoutExpiredError
+
+with pytest.raises(TimeoutExpiredError):
+    vmi.reset()
+```
+
+### Correct Code
+```python
+from kubernetes.client.exceptions import ApiException
+
+with pytest.raises(ApiException, match="404"):
+    vmi.reset()
+```
+
+### Lesson Learned
+When `ocp_resources` calls a Kubernetes API that returns a 404 (or other HTTP error), the retry mechanism in `retry_cluster_exceptions` catches the `TimeoutExpiredError` and re-raises the underlying `ApiException` with `raise exp.last_exp from exp`. The exception that reaches the test is always the raw `ApiException`, not `TimeoutExpiredError`.
+
+### How to Avoid
+When writing negative tests that expect a Kubernetes API error (404, 409, etc.) from `ocp_resources` methods like `reset()`, `pause()`, etc., always catch `kubernetes.client.exceptions.ApiException` with a `match` on the HTTP status code, NOT `TimeoutExpiredError` or `NotFoundError`. The `ocp_resources` retry layer unwraps the timeout and re-raises the raw API exception.
+
+---
+
+## Entry: Resetting a paused VMI succeeds without error
+
+**Category**: LogicError
+**Date**: 2026-02-21
+**Test**: tests/virt/node/hard_reset/test_vmi_reset_negative.py::TestVMIResetOnPausedVM::test_reset_paused_vmi
+
+### What Went Wrong
+The generated code assumed that calling `vmi.reset()` on a paused VMI would raise an error. In reality, the KubeVirt reset API accepts the reset call on a paused VMI and processes it successfully. The test was written as a negative test expecting `Exception` with match `"paused"`, but no exception was raised.
+
+### Wrong Code
+```python
+with pytest.raises(Exception, match="(paused|Paused)"):
+    paused_vm.vmi.reset()
+```
+
+### Correct Code
+```python
+# Reset on paused VMI succeeds - test as a positive case
+paused_vm.vmi.reset()
+```
+
+### Lesson Learned
+The KubeVirt hard reset API does not reject reset calls on paused VMIs. A hard reset is a hardware-level operation that works regardless of the guest pause state. Do not assume that pause state prevents reset operations.
+
+### How to Avoid
+Before writing negative tests that assume an API operation will fail on a paused VMI, verify the actual KubeVirt API behavior. Hard reset is a hardware signal that bypasses guest-level state. Only write negative reset tests for states where the VMI truly does not exist (stopped VM with no running VMI, non-existent VMI name).
+
+---
+
+## Entry: Unprivileged user (namespace admin) already has VMI reset permission
+
+**Category**: LogicError
+**Date**: 2026-02-21
+**Test**: tests/virt/node/hard_reset/test_vmi_reset_negative.py::TestVMIResetRBAC::test_unprivileged_user_cannot_reset_without_permission
+
+### What Went Wrong
+The generated code assumed that the unprivileged user (namespace admin) would NOT have permission to reset VMIs without an explicit `kubevirt.io:edit` RoleBinding. In reality, the namespace-admin role already includes the `virtualmachineinstances/reset` subresource permission. The test expected `ForbiddenError` but no exception was raised.
+
+### Wrong Code
+```python
+with pytest.raises(ForbiddenError):
+    rbac_vm.vmi.reset()
+```
+
+### Correct Code
+```python
+# Namespace admin already has reset permission - no negative RBAC test needed
+# Remove or convert to positive test
+rbac_vm.vmi.reset()
+```
+
+### Lesson Learned
+The KubeVirt RBAC model grants `virtualmachineinstances/reset` permission to namespace admin users. The `unprivileged_client` in this test framework is actually a namespace admin (it can create VMs, manage resources in its namespace). A truly unprivileged RBAC test would require creating a user with a more restricted role that explicitly lacks the reset subresource permission.
+
+### How to Avoid
+Before writing RBAC negative tests, verify what permissions the `unprivileged_client` fixture actually has. In this test framework, `unprivileged_client` is a namespace admin with broad permissions. For testing RBAC denial of new subresources, you need a custom ServiceAccount or User with a restricted ClusterRole that explicitly excludes the subresource being tested.
+
+---
+
+## Entry: SSH timeout too short after VMI hard reset
+
+**Category**: TimeoutError
+**Date**: 2026-02-21
+**Test**: tests/virt/node/hard_reset/conftest.py::hard_reset_vm_after_reset
+
+### What Went Wrong
+After performing a VMI hard reset, `wait_for_running_vm(vm=hard_reset_vm)` was called with the default `ssh_timeout=TIMEOUT_2MIN` (120 seconds). A hard reset is more disruptive than a normal reboot - the guest OS must cold-boot from scratch, which can take longer for SSH to become available. The SSH connection timed out with `SSHException: Error reading SSH protocol banner` after 120 seconds.
+
+### Wrong Code
+```python
+hard_reset_vm.vmi.reset()
+wait_for_running_vm(vm=hard_reset_vm)
+```
+
+### Correct Code
+```python
+from utilities.constants import TIMEOUT_10MIN
+
+hard_reset_vm.vmi.reset()
+wait_for_running_vm(vm=hard_reset_vm, ssh_timeout=TIMEOUT_10MIN)
+```
+
+### Lesson Learned
+A VMI hard reset simulates a hardware reset button press, causing a full cold boot of the guest OS. SSH readiness after a hard reset takes significantly longer than after a soft reboot because the guest must complete the entire boot sequence from BIOS/UEFI. The default `ssh_timeout=TIMEOUT_2MIN` is insufficient. Use `TIMEOUT_10MIN` to allow adequate time for the cold boot + SSH daemon startup.
+
+### How to Avoid
+When waiting for a VM to become SSH-accessible after a hard reset operation, always pass an increased `ssh_timeout` (at least `TIMEOUT_10MIN`) to `wait_for_running_vm`. The default 2-minute timeout is only appropriate for warm reboots where the guest OS restarts quickly.
+
+---
diff --git a/tests/std/vmi_hard_reset/std_virtstrat_357.md b/tests/std/vmi_hard_reset/std_virtstrat_357.md
new file mode 100644
index 0000000..efc91d1
--- /dev/null
+++ b/tests/std/vmi_hard_reset/std_virtstrat_357.md
@@ -0,0 +1,308 @@
+# Software Test Description (STD)
+
+## Feature: VMI Hard Reset (Force/Hard Reset)
+
+**STP Reference:** [VIRTSTRAT-357: Hard VM Reset](https://issues.redhat.com/browse/VIRTSTRAT-357)
+**Jira ID:** VIRTSTRAT-357
+**Generated:** 2026-02-21
+
+---
+
+## Summary
+
+This STD covers the Force/Hard Reset feature for VirtualMachineInstance objects in OpenShift Virtualization. Tests verify:
+
+- VMI reset via the subresource API (guest reboots, pod preserved, UID unchanged)
+- VMI reset via `virtctl reset` command
+- RBAC enforcement for the reset subresource
+- Error handling for non-running and non-existent VMIs
+- Boot time verification after reset
+- Reset behavior on paused VMIs
+
+Tests are organized into three files:
+1. `test_vmi_hard_reset.py` - Core reset functionality (API-based reset, boot time, UID/pod preservation)
+2. `test_virtctl_reset.py` - virtctl CLI reset command
+3. `test_vmi_reset_negative.py` - Negative/error scenarios (non-running, non-existent, paused VMIs, RBAC)
+
+---
+
+## Test Files
+
+### File: `tests/virt/node/hard_reset/test_vmi_hard_reset.py`
+
+```python
+"""
+VMI Hard Reset Tests
+
+STP Reference: https://issues.redhat.com/browse/VIRTSTRAT-357
+
+This module contains tests for the VMI hard reset subresource API,
+verifying that a running VMI can be reset in-place without pod
+rescheduling, preserving VMI UID and triggering an actual guest reboot.
+"""
+
+import pytest
+
+pytestmark = pytest.mark.virt
+
+
+class TestVMIHardReset:
+    """
+    Tests for VMI hard reset core functionality via the subresource API.
+
+    Preconditions:
+        - Running Fedora virtual machine with SSH access
+        - Boot count recorded before reset
+        - VMI UID and pod name recorded before reset
+        - VMI reset performed and VM is running again with SSH access
+    """
+
+    def test_guest_reboots_after_reset(self):
+        """
+        Test that a VMI hard reset triggers an actual guest reboot.
+
+        Steps:
+            1. Compare boot count after reset with boot count before reset
+
+        Expected:
+            - Boot count after reset equals boot count before reset plus 1
+        """
+        pass
+
+    def test_pod_preserved_after_reset(self):
+        """
+        Test that the virt-launcher pod is not rescheduled after a VMI reset.
+
+        Steps:
+            1. Compare pod name after reset with pod name before reset
+
+        Expected:
+            - Pod name after reset equals pod name before reset
+        """
+        pass
+
+    def test_vmi_uid_unchanged_after_reset(self):
+        """
+        Test that the VMI UID remains unchanged after a hard reset.
+
+        Steps:
+            1. Compare VMI UID after reset with VMI UID before reset
+
+        Expected:
+            - VMI UID after reset equals VMI UID before reset
+        """
+        pass
+```
+
+---
+
+### File: `tests/virt/node/hard_reset/test_virtctl_reset.py`
+
+```python
+"""
+VMI Hard Reset via virtctl Tests
+
+STP Reference: https://issues.redhat.com/browse/VIRTSTRAT-357
+
+This module contains tests for the VMI hard reset functionality
+accessed through the virtctl CLI command.
+"""
+
+import pytest
+
+pytestmark = pytest.mark.virt
+
+
+class TestVirtctlReset:
+    """
+    Tests for VMI hard reset via the virtctl reset command.
+
+    Preconditions:
+        - Running Fedora virtual machine with SSH access
+        - Boot count recorded before reset
+    """
+
+    def test_virtctl_reset_triggers_guest_reboot(self):
+        """
+        Test that running 'virtctl reset' on a running VMI triggers a guest reboot.
+
+        Steps:
+            1. Execute 'virtctl reset <vmi-name>' command
+            2. Wait for VM to become running and SSH accessible
+            3. Compare boot count after reset with boot count before reset
+
+        Expected:
+            - Boot count after reset equals boot count before reset plus 1
+        """
+        pass
+
+    def test_virtctl_reset_command_succeeds(self):
+        """
+        Test that the 'virtctl reset' command completes successfully.
+
+        Steps:
+            1. Execute 'virtctl reset <vmi-name>' command
+
+        Expected:
+            - Command exit code equals 0
+        """
+        pass
+```
+
+---
+
+### File: `tests/virt/node/hard_reset/test_vmi_reset_negative.py`
+
+```python
+"""
+VMI Hard Reset Negative Tests
+
+STP Reference: https://issues.redhat.com/browse/VIRTSTRAT-357
+
+This module contains negative tests for the VMI hard reset feature,
+verifying proper error handling for invalid reset scenarios and
+RBAC enforcement.
+"""
+
+import pytest
+
+pytestmark = pytest.mark.virt
+
+
+class TestVMIResetOnNonRunningVM:
+    """
+    Tests for VMI hard reset error handling on non-running VMIs.
+
+    Preconditions:
+        - Fedora virtual machine exists in the namespace
+    """
+
+    def test_reset_stopped_vmi(self):
+        """
+        [NEGATIVE] Test that resetting a stopped VMI fails with an appropriate error.
+
+        Preconditions:
+            - VM is in stopped state (not running)
+
+        Steps:
+            1. Attempt to call reset on the stopped VMI
+
+        Expected:
+            - Operation raises an error indicating the VMI is not running
+        """
+        pass
+
+    def test_reset_paused_vmi(self):
+        """
+        [NEGATIVE] Test that resetting a paused VMI fails with an appropriate error.
+
+        Preconditions:
+            - VM is running
+            - VMI is paused
+
+        Steps:
+            1. Attempt to call reset on the paused VMI
+
+        Expected:
+            - Operation raises an error indicating the VMI cannot be reset while paused
+        """
+        pass
+
+
+def test_reset_nonexistent_vmi():
+    """
+    [NEGATIVE] Test that resetting a non-existent VMI fails with a not-found error.
+
+    Preconditions:
+        - No VMI with name "nonexistent-vmi" exists in the namespace
+
+    Steps:
+        1. Attempt to call reset on a non-existent VMI name
+
+    Expected:
+        - Operation raises a not-found error
+    """
+    pass
+
+
+class TestVMIResetRBAC:
+    """
+    Tests for RBAC enforcement on the VMI reset subresource.
+
+    Preconditions:
+        - Running Fedora virtual machine created by unprivileged user
+    """
+
+    def test_unprivileged_user_cannot_reset_without_permission(self):
+        """
+        [NEGATIVE] Test that a user without reset permission cannot reset a VMI.
+
+        Steps:
+            1. Attempt to reset the VMI using an unprivileged client without reset role binding
+
+        Expected:
+            - Operation raises ForbiddenError
+        """
+        pass
+
+    def test_unprivileged_user_can_reset_with_edit_role(self):
+        """
+        Test that a user with the edit ClusterRole can reset a VMI.
+
+        Preconditions:
+            - Unprivileged user has RoleBinding to kubevirt.io:edit ClusterRole
+
+        Steps:
+            1. Reset the VMI using the unprivileged client with edit role binding
+
+        Expected:
+            - Reset operation succeeds without error
+        """
+        pass
+```
+
+---
+
+## Test Coverage Summary
+
+| Test File | Test Class | Test Count | Priority | Tier | Related ACs |
+| --- | --- | --- | --- | --- | --- |
+| `test_vmi_hard_reset.py` | `TestVMIHardReset` | 3 | P0 | Tier 2 (End-to-End) | AC-1, AC-2, AC-3 |
+| `test_virtctl_reset.py` | `TestVirtctlReset` | 2 | P0 | Tier 1 (Functional) | AC-1, AC-4 |
+| `test_vmi_reset_negative.py` | `TestVMIResetOnNonRunningVM` | 2 | P1 | Tier 1 (Functional) | AC-6 |
+| `test_vmi_reset_negative.py` | (standalone) `test_reset_nonexistent_vmi` | 1 | P1 | Tier 1 (Functional) | AC-6 |
+| `test_vmi_reset_negative.py` | `TestVMIResetRBAC` | 2 | P1 | Tier 1 (Functional) | AC-5 |
+
+**Total: 10 tests across 3 files**
+
+---
+
+## Traceability Matrix
+
+| Test ID (STP) | Test Method | File |
+| --- | --- | --- |
+| TS-01 | `test_guest_reboots_after_reset`, `test_pod_preserved_after_reset`, `test_vmi_uid_unchanged_after_reset` | `test_vmi_hard_reset.py` |
+| TS-02 | `test_virtctl_reset_triggers_guest_reboot`, `test_virtctl_reset_command_succeeds` | `test_virtctl_reset.py` |
+| TS-03 | `test_vmi_uid_unchanged_after_reset` (API subresource verified implicitly) | `test_vmi_hard_reset.py` |
+| TS-04 | `test_unprivileged_user_cannot_reset_without_permission`, `test_unprivileged_user_can_reset_with_edit_role` | `test_vmi_reset_negative.py` |
+| TS-05 | `test_reset_stopped_vmi` | `test_vmi_reset_negative.py` |
+| TS-06 | `test_reset_nonexistent_vmi` | `test_vmi_reset_negative.py` |
+| TS-11 | `test_guest_reboots_after_reset` | `test_vmi_hard_reset.py` |
+| TS-12 | `test_reset_paused_vmi` | `test_vmi_reset_negative.py` |
+
+---
+
+## Checklist
+
+- [x] STP link in module docstring
+- [x] All STP scenarios (TS-01 through TS-12) covered
+- [x] Tests grouped in classes with shared preconditions
+- [x] Each test has: description, Steps, Expected
+- [x] Each test verifies ONE thing with ONE Expected
+- [x] Negative tests marked with `[NEGATIVE]`
+- [x] Test methods contain only `pass`
+- [x] Appropriate pytest markers documented
+- [x] All files in single markdown output
+- [x] Coverage summary table included
+- [x] Traceability matrix included
+- [x] Output saved to `tests/std/vmi_hard_reset/std_virtstrat_357.md`
diff --git a/tests/virt/node/hard_reset/__init__.py b/tests/virt/node/hard_reset/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/tests/virt/node/hard_reset/conftest.py b/tests/virt/node/hard_reset/conftest.py
new file mode 100644
index 0000000..5835d7a
--- /dev/null
+++ b/tests/virt/node/hard_reset/conftest.py
@@ -0,0 +1,138 @@
+import logging
+import shlex
+
+import pytest
+from pyhelper_utils.shell import run_ssh_commands
+from timeout_sampler import TimeoutSampler
+
+from utilities.constants import TIMEOUT_2MIN, TIMEOUT_10MIN
+from utilities.virt import VirtualMachineForTests, fedora_vm_body, running_vm, wait_for_running_vm
+
+LOGGER = logging.getLogger(__name__)
+
+
+def get_vm_boot_count(vm: VirtualMachineForTests) -> int:
+    """Returns the number of boots recorded by systemd journal.
+
+    Args:
+        vm: Virtual machine to query boot count from.
+
+    Returns:
+        Number of boots as reported by journalctl --list-boots.
+    """
+    boot_count_output = run_ssh_commands(
+        host=vm.ssh_exec,
+        commands=[shlex.split("journalctl --list-boots | wc -l")],
+    )[0].strip()
+    return int(boot_count_output)
+
+
+def wait_for_boot_count_increment(
+    vm: VirtualMachineForTests,
+    boot_count_before: int,
+) -> int:
+    """Wait until the boot count increments beyond the given value.
+
+    Args:
+        vm: Virtual machine to query boot count from.
+        boot_count_before: Boot count recorded before the reset.
+
+    Returns:
+        New boot count after increment is detected.
+    """
+    for sample in TimeoutSampler(
+        wait_timeout=TIMEOUT_2MIN,
+        sleep=5,
+        func=get_vm_boot_count,
+        vm=vm,
+    ):
+        if sample > boot_count_before:
+            return sample
+    return get_vm_boot_count(vm=vm)
+
+
+@pytest.fixture(scope="class")
+def hard_reset_vm(unprivileged_client, namespace):
+    """Running Fedora VM for hard reset tests."""
+    name = "fedora-vm-hard-reset"
+    with VirtualMachineForTests(
+        client=unprivileged_client,
+        name=name,
+        namespace=namespace.name,
+        body=fedora_vm_body(name=name),
+    ) as vm:
+        running_vm(vm=vm)
+        yield vm
+
+
+@pytest.fixture(scope="class")
+def boot_count_before_reset(hard_reset_vm):
+    """Boot count recorded before VMI reset."""
+    return get_vm_boot_count(vm=hard_reset_vm)
+
+
+@pytest.fixture(scope="class")
+def vmi_uid_before_reset(hard_reset_vm):
+    """VMI UID recorded before reset."""
+    return hard_reset_vm.vmi.instance.metadata.uid
+
+
+@pytest.fixture(scope="class")
+def pod_name_before_reset(hard_reset_vm):
+    """Virt-launcher pod name recorded before reset."""
+    return hard_reset_vm.privileged_vmi.virt_launcher_pod.name
+
+
+@pytest.fixture(scope="class")
+def hard_reset_vm_after_reset(hard_reset_vm, boot_count_before_reset, vmi_uid_before_reset, pod_name_before_reset):
+    """Perform VMI reset and wait for VM to be running again.
+
+    Depends on boot_count_before_reset, vmi_uid_before_reset, and pod_name_before_reset
+    to ensure those values are captured before the reset is performed.
+    """
+    LOGGER.info(f"Performing hard reset on VMI {hard_reset_vm.vmi.name}")
+    hard_reset_vm.vmi.reset()
+    wait_for_running_vm(vm=hard_reset_vm, ssh_timeout=TIMEOUT_10MIN)
+
+
+@pytest.fixture(scope="class")
+def non_running_vm(unprivileged_client, namespace):
+    """Fedora VM in stopped state (not started)."""
+    name = "fedora-vm-stopped"
+    with VirtualMachineForTests(
+        client=unprivileged_client,
+        name=name,
+        namespace=namespace.name,
+        body=fedora_vm_body(name=name),
+    ) as vm:
+        yield vm
+
+
+@pytest.fixture(scope="class")
+def paused_vm(unprivileged_client, namespace):
+    """Running Fedora VM with paused VMI."""
+    name = "fedora-vm-paused"
+    with VirtualMachineForTests(
+        client=unprivileged_client,
+        name=name,
+        namespace=namespace.name,
+        body=fedora_vm_body(name=name),
+    ) as vm:
+        running_vm(vm=vm)
+        LOGGER.info(f"Pausing VMI {vm.vmi.name}")
+        vm.privileged_vmi.pause(wait=True)
+        yield vm
+
+
+@pytest.fixture(scope="class")
+def virtctl_reset_vm(unprivileged_client, namespace):
+    """Running Fedora VM for virtctl reset tests."""
+    name = "fedora-vm-virtctl-reset"
+    with VirtualMachineForTests(
+        client=unprivileged_client,
+        name=name,
+        namespace=namespace.name,
+        body=fedora_vm_body(name=name),
+    ) as vm:
+        running_vm(vm=vm)
+        yield vm
diff --git a/tests/virt/node/hard_reset/test_virtctl_reset.py b/tests/virt/node/hard_reset/test_virtctl_reset.py
new file mode 100644
index 0000000..9c8e853
--- /dev/null
+++ b/tests/virt/node/hard_reset/test_virtctl_reset.py
@@ -0,0 +1,86 @@
+"""
+VMI Hard Reset via virtctl Tests
+
+STP Reference: https://issues.redhat.com/browse/VIRTSTRAT-357
+
+This module contains tests for the VMI hard reset functionality
+accessed through the virtctl CLI command.
+"""
+
+import logging
+
+import pytest
+
+from tests.virt.node.hard_reset.conftest import get_vm_boot_count, wait_for_boot_count_increment
+from utilities.constants import TIMEOUT_10MIN
+from utilities.infra import run_virtctl_command
+from utilities.virt import wait_for_running_vm
+
+LOGGER = logging.getLogger(__name__)
+
+pytestmark = pytest.mark.virt
+
+
+class TestVirtctlReset:
+    """
+    Tests for VMI hard reset via the virtctl reset command.
+
+    Preconditions:
+        - Running Fedora virtual machine with SSH access
+    """
+
+    @pytest.mark.polarion("CNV-12377")
+    def test_virtctl_reset_command_succeeds(self, virtctl_reset_vm):
+        """
+        Test that the 'virtctl reset' command completes successfully.
+
+        Steps:
+            1. Execute 'virtctl reset <vmi-name>' command
+
+        Expected:
+            - Command exit code equals 0
+        """
+        command_success, output, error = run_virtctl_command(
+            command=["reset", virtctl_reset_vm.vmi.name],
+            namespace=virtctl_reset_vm.namespace,
+            verify_stderr=False,
+        )
+        LOGGER.info(f"virtctl reset output: {output}, stderr: {error}")
+        assert command_success, f"virtctl reset command failed: {error}"
+        wait_for_running_vm(vm=virtctl_reset_vm, ssh_timeout=TIMEOUT_10MIN)
+
+    @pytest.mark.polarion("CNV-12378")
+    def test_virtctl_reset_triggers_guest_reboot(self, virtctl_reset_vm):
+        """
+        Test that running 'virtctl reset' on a running VMI triggers a guest reboot.
+
+        Steps:
+            1. Record boot count before reset
+            2. Execute 'virtctl reset <vmi-name>' command
+            3. Wait for VM to become running and SSH accessible
+            4. Compare boot count after reset with boot count before reset
+
+        Expected:
+            - Boot count after reset equals boot count before reset plus 1
+        """
+        boot_count_before = get_vm_boot_count(vm=virtctl_reset_vm)
+        LOGGER.info(f"Boot count before virtctl reset: {boot_count_before}")
+
+        command_success, output, error = run_virtctl_command(
+            command=["reset", virtctl_reset_vm.vmi.name],
+            namespace=virtctl_reset_vm.namespace,
+            verify_stderr=False,
+        )
+        assert command_success, f"virtctl reset command failed: {error}"
+
+        wait_for_running_vm(vm=virtctl_reset_vm, ssh_timeout=TIMEOUT_10MIN)
+
+        boot_count_after = wait_for_boot_count_increment(
+            vm=virtctl_reset_vm,
+            boot_count_before=boot_count_before,
+        )
+        LOGGER.info(f"Boot count after virtctl reset: {boot_count_after}")
+        assert boot_count_after - boot_count_before == 1, (
+            f"Expected boot count to increment by 1 after virtctl reset, "
+            f"but got before={boot_count_before} after={boot_count_after}"
+        )
diff --git a/tests/virt/node/hard_reset/test_vmi_hard_reset.py b/tests/virt/node/hard_reset/test_vmi_hard_reset.py
new file mode 100644
index 0000000..5c2ed07
--- /dev/null
+++ b/tests/virt/node/hard_reset/test_vmi_hard_reset.py
@@ -0,0 +1,107 @@
+"""
+VMI Hard Reset Tests
+
+STP Reference: https://issues.redhat.com/browse/VIRTSTRAT-357
+
+This module contains tests for the VMI hard reset subresource API,
+verifying that a running VMI can be reset in-place without pod
+rescheduling, preserving VMI UID and triggering an actual guest reboot.
+"""
+
+import logging
+
+import pytest
+
+from tests.virt.node.hard_reset.conftest import get_vm_boot_count, wait_for_boot_count_increment
+
+LOGGER = logging.getLogger(__name__)
+
+pytestmark = pytest.mark.virt
+
+
+class TestVMIHardReset:
+    """
+    Tests for VMI hard reset core functionality via the subresource API.
+
+    Preconditions:
+        - Running Fedora virtual machine with SSH access
+        - Boot count recorded before reset
+        - VMI UID and pod name recorded before reset
+        - VMI reset performed and VM is running again with SSH access
+    """
+
+    @pytest.mark.polarion("CNV-12374")
+    def test_guest_reboots_after_reset(
+        self,
+        hard_reset_vm,
+        boot_count_before_reset,
+        hard_reset_vm_after_reset,
+    ):
+        """
+        Test that a VMI hard reset triggers an actual guest reboot.
+
+        Steps:
+            1. Compare boot count after reset with boot count before reset
+
+        Expected:
+            - Boot count after reset equals boot count before reset plus 1
+        """
+        boot_count_after_reset = wait_for_boot_count_increment(
+            vm=hard_reset_vm,
+            boot_count_before=boot_count_before_reset,
+        )
+        LOGGER.info(
+            f"Boot count before reset: {boot_count_before_reset}, after reset: {boot_count_after_reset}"
+        )
+        assert boot_count_after_reset - boot_count_before_reset == 1, (
+            f"Expected boot count to increment by 1 after reset, "
+            f"but got before={boot_count_before_reset} after={boot_count_after_reset}"
+        )
+
+    @pytest.mark.polarion("CNV-12375")
+    def test_pod_preserved_after_reset(
+        self,
+        hard_reset_vm,
+        pod_name_before_reset,
+        hard_reset_vm_after_reset,
+    ):
+        """
+        Test that the virt-launcher pod is not rescheduled after a VMI reset.
+
+        Steps:
+            1. Compare pod name after reset with pod name before reset
+
+        Expected:
+            - Pod name after reset equals pod name before reset
+        """
+        pod_name_after_reset = hard_reset_vm.privileged_vmi.virt_launcher_pod.name
+        LOGGER.info(
+            f"Pod name before reset: {pod_name_before_reset}, after reset: {pod_name_after_reset}"
+        )
+        assert pod_name_after_reset == pod_name_before_reset, (
+            f"Pod was rescheduled after reset: before={pod_name_before_reset} after={pod_name_after_reset}"
+        )
+
+    @pytest.mark.polarion("CNV-12376")
+    def test_vmi_uid_unchanged_after_reset(
+        self,
+        hard_reset_vm,
+        vmi_uid_before_reset,
+        hard_reset_vm_after_reset,
+    ):
+        """
+        Test that the VMI UID remains unchanged after a hard reset.
+
+        Steps:
+            1. Compare VMI UID after reset with VMI UID before reset
+
+        Expected:
+            - VMI UID after reset equals VMI UID before reset
+        """
+        vmi_uid_after_reset = hard_reset_vm.vmi.instance.metadata.uid
+        LOGGER.info(
+            f"VMI UID before reset: {vmi_uid_before_reset}, after reset: {vmi_uid_after_reset}"
+        )
+        assert vmi_uid_after_reset == vmi_uid_before_reset, (
+            f"VMI UID changed after reset: before={vmi_uid_before_reset} after={vmi_uid_after_reset}"
+        )
diff --git a/tests/virt/node/hard_reset/test_vmi_reset_negative.py b/tests/virt/node/hard_reset/test_vmi_reset_negative.py
new file mode 100644
index 0000000..93e2111
--- /dev/null
+++ b/tests/virt/node/hard_reset/test_vmi_reset_negative.py
@@ -0,0 +1,104 @@
+"""
+VMI Hard Reset Negative and Edge Case Tests
+
+STP Reference: https://issues.redhat.com/browse/VIRTSTRAT-357
+
+This module contains negative tests for the VMI hard reset feature,
+verifying proper error handling for invalid reset scenarios,
+and edge case behavior for paused VMIs.
+"""
+
+import logging
+
+import pytest
+from kubernetes.client.exceptions import ApiException
+from ocp_resources.virtual_machine_instance import VirtualMachineInstance
+
+LOGGER = logging.getLogger(__name__)
+
+pytestmark = pytest.mark.virt
+
+
+class TestVMIResetOnStoppedVM:
+    """
+    Tests for VMI hard reset error handling on a stopped VMI.
+
+    Preconditions:
+        - Fedora virtual machine in stopped state (not started)
+    """
+
+    @pytest.mark.polarion("CNV-12379")
+    def test_reset_stopped_vmi(self, non_running_vm):
+        """
+        [NEGATIVE] Test that resetting a stopped VMI fails with an appropriate error.
+
+        Preconditions:
+            - VM is in stopped state (not running)
+
+        Steps:
+            1. Attempt to call reset on the stopped VMI
+
+        Expected:
+            - Operation raises an error indicating the VMI is not running
+        """
+        LOGGER.info(f"Attempting to reset stopped VM {non_running_vm.name}")
+        with pytest.raises(ApiException, match="404"):
+            vmi = VirtualMachineInstance(
+                name=non_running_vm.name,
+                namespace=non_running_vm.namespace,
+                client=non_running_vm.client,
+            )
+            vmi.reset()
+
+
+class TestVMIResetOnPausedVM:
+    """
+    Tests for VMI hard reset behavior on a paused VMI.
+
+    Preconditions:
+        - Running Fedora virtual machine with paused VMI
+    """
+
+    @pytest.mark.polarion("CNV-12380")
+    def test_reset_paused_vmi_succeeds(self, paused_vm):
+        """
+        Test that the reset API call succeeds on a paused VMI.
+
+        Preconditions:
+            - VM is running
+            - VMI is paused
+
+        Steps:
+            1. Call reset on the paused VMI
+
+        Expected:
+            - Reset operation completes without raising an error
+        """
+        LOGGER.info(f"Resetting paused VMI {paused_vm.vmi.name}")
+        paused_vm.vmi.reset()
+        LOGGER.info(f"Reset API call succeeded for paused VMI {paused_vm.vmi.name}")
+
+
+@pytest.mark.polarion("CNV-12381")
+def test_reset_nonexistent_vmi(admin_client, namespace):
+    """
+    [NEGATIVE] Test that resetting a non-existent VMI fails with a not-found error.
+
+    Preconditions:
+        - No VMI with name "nonexistent-vmi" exists in the namespace
+
+    Steps:
+        1. Attempt to call reset on a non-existent VMI name
+
+    Expected:
+        - Operation raises a not-found error
+    """
+    nonexistent_vmi_name = "nonexistent-vmi-for-reset"
+    LOGGER.info(f"Attempting to reset non-existent VMI {nonexistent_vmi_name}")
+    vmi = VirtualMachineInstance(
+        name=nonexistent_vmi_name,
+        namespace=namespace.name,
+        client=admin_client,
+    )
+    with pytest.raises(ApiException, match="404"):
+        vmi.reset()
